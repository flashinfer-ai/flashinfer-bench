{
  "name": "nsa_topk_indexer_h16_ckv512_kpe64_topk256_ps64",
  "description": "Native Sparse Attention (NSA) TopK indexer for DeepSeek-V3.2. Computes sparse attention scores and selects top-K KV cache indices for each query token. Page size 64 variant.",
  "op_type": "nsa_paged",
  "tags": [
    "stage:indexer",
    "status:draft",
    "model:deepseek-v3.2",
    "sparse:topk"
  ],
  "axes": {
    "batch_size": {
      "type": "var"
    },
    "num_index_heads": {
      "type": "const",
      "value": 16,
      "description": "Number of indexer heads (typically matches query heads)."
    },
    "index_head_dim": {
      "type": "const",
      "value": 64,
      "description": "Indexer head dimension."
    },
    "page_size": {
      "type": "const",
      "value": 64,
      "description": "Page size for KV cache (64 tokens per page)."
    },
    "topk": {
      "type": "const",
      "value": 256,
      "description": "Number of top-K indices to select."
    },
    "max_seq_len": {
      "type": "var",
      "description": "Maximum sequence length for indexing."
    },
    "num_pages": {
      "type": "var",
      "description": "Total number of allocated pages in the KV cache."
    }
  },
  "constraints": [
    "topk <= max_seq_len"
  ],
  "inputs": {
    "q_index": {
      "shape": [
        "batch_size",
        "num_index_heads",
        "index_head_dim"
      ],
      "dtype": "bfloat16",
      "description": "Query tensor for indexing (low-rank compressed)."
    },
    "k_index_cache": {
      "shape": [
        "num_pages",
        "page_size",
        "index_head_dim"
      ],
      "dtype": "bfloat16",
      "description": "Key index cache for sparse attention scoring."
    },
    "seq_lens": {
      "shape": [
        "batch_size"
      ],
      "dtype": "int32",
      "description": "Sequence lengths for each batch element."
    },
    "page_table": {
      "shape": [
        "batch_size",
        "max_seq_len"
      ],
      "dtype": "int32",
      "description": "Page table mapping sequence positions to KV cache pages. For page_size=64, encodes (page_idx * 64 + offset)."
    }
  },
  "outputs": {
    "topk_indices": {
      "shape": [
        "batch_size",
        "topk"
      ],
      "dtype": "int32",
      "description": "Top-K page indices for each batch element. Values of -1 indicate padding. For page_size=64, indices encode (page_idx * 64 + offset)."
    },
    "topk_scores": {
      "shape": [
        "batch_size",
        "topk"
      ],
      "dtype": "float32",
      "description": "Attention scores for the selected top-K indices."
    }
  },
  "reference": "import torch\n\n\n@torch.no_grad()\ndef run(q_index, k_index_cache, seq_lens, page_table):\n    batch_size, num_index_heads, index_head_dim = q_index.shape\n    num_pages, page_size, _ = k_index_cache.shape\n    topk = 256\n\n    # Check constants\n    assert num_index_heads == 16\n    assert index_head_dim == 64\n    assert page_size == 64\n\n    device = q_index.device\n\n    # Flatten paged index cache: [num_pages, page_size, dim] -> [num_pages * page_size, dim]\n    K_all = k_index_cache.reshape(-1, index_head_dim).to(torch.float32)  # [total_kv_tokens, index_head_dim]\n\n    topk_indices = torch.full((batch_size, topk), -1, dtype=torch.int32, device=device)\n    topk_scores = torch.full((batch_size, topk), -float(\"inf\"), dtype=torch.float32, device=device)\n\n    for b in range(batch_size):\n        seq_len = int(seq_lens[b].item())\n        \n        if seq_len == 0:\n            continue\n\n        # Get token indices for this sequence (page_table encodes page_idx * 64 + offset)\n        token_indices = page_table[b, :seq_len].to(torch.long)  # [seq_len]\n        K = K_all[token_indices]  # [seq_len, index_head_dim]\n        \n        # Query for this batch element\n        q = q_index[b].to(torch.float32)  # [num_index_heads, index_head_dim]\n        \n        # Compute attention scores\n        scores = q @ K.T  # [num_index_heads, seq_len]\n        \n        # Average across heads for TopK selection\n        avg_scores = scores.mean(dim=0)  # [seq_len]\n        \n        # Select top-K\n        actual_topk = min(topk, seq_len)\n        topk_vals, topk_idx = torch.topk(avg_scores, actual_topk)\n        \n        # Convert local indices to global token indices\n        topk_tokens = token_indices[topk_idx]  # [actual_topk]\n        \n        topk_indices[b, :actual_topk] = topk_tokens.to(torch.int32)\n        topk_scores[b, :actual_topk] = topk_vals\n\n    return {\"topk_indices\": topk_indices, \"topk_scores\": topk_scores}"
}
