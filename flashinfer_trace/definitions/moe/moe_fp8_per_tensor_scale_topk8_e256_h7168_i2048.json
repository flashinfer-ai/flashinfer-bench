{
  "name": "moe_fp8_per_tensor_scale_topk8_e256_h7168_i2048",
  "description": "FP8 per-tensor scale MoE operation with Renormalize routing (TopK -> Softmax). Per-tensor quantization means one scale factor per tensor (or per-expert for weights).",
  "op_type": "moe",
  "tags": [
    "status:verified",
    "model:llama4",
    "quantization:float8_e4m3fn"
  ],
  "axes": {
    "seq_len": {
      "type": "var",
      "description": "Sequence length (number of tokens)"
    },
    "num_experts": {
      "type": "const",
      "value": 256,
      "description": "Total number of experts."
    },
    "num_local_experts": {
      "type": "const",
      "value": 32,
      "description": "Number of local experts with EP size 8."
    },
    "hidden_size": {
      "type": "const",
      "value": 7168,
      "description": "Hidden dimension size."
    },
    "intermediate_size": {
      "type": "const",
      "value": 2048,
      "description": "MoE intermediate layer size."
    },
    "gemm1_out_size": {
      "type": "const",
      "value": 4096,
      "description": "Output size of the first GEMM (W13). Should be 2 * intermediate_size"
    }
  },
  "inputs": {
    "routing_logits": {
      "shape": [
        "seq_len",
        "num_experts"
      ],
      "dtype": "bfloat16",
      "description": "Tensor of routing logits for expert selection"
    },
    "routing_bias": {
      "shape": [
        "num_experts"
      ],
      "dtype": "bfloat16",
      "description": "Bias tensor for routing. Pass all zeros for no bias."
    },
    "hidden_states": {
      "shape": [
        "seq_len",
        "hidden_size"
      ],
      "dtype": "float8_e4m3fn",
      "description": "Input hidden states tensor (FP8 quantized)"
    },
    "hidden_states_scale": {
      "shape": null,
      "dtype": "float32",
      "description": "Global scale factor for hidden states (per-tensor scale)."
    },
    "gemm1_weights": {
      "shape": [
        "num_local_experts",
        "gemm1_out_size",
        "hidden_size"
      ],
      "dtype": "float8_e4m3fn",
      "description": "First GEMM weights for all local experts (gate and up projections)."
    },
    "gemm1_scales": {
      "shape": [
        "num_local_experts"
      ],
      "dtype": "float32",
      "description": "Per-expert global scale factors for first GEMM weights."
    },
    "gemm2_weights": {
      "shape": [
        "num_local_experts",
        "hidden_size",
        "intermediate_size"
      ],
      "dtype": "float8_e4m3fn",
      "description": "Second GEMM weights for all local experts (down projection)."
    },
    "gemm2_scales": {
      "shape": [
        "num_local_experts"
      ],
      "dtype": "float32",
      "description": "Per-expert global scale factors for second GEMM weights."
    },
    "local_expert_offset": {
      "shape": null,
      "dtype": "int32",
      "description": "Offset of local experts in global expert space."
    },
    "routed_scaling_factor": {
      "shape": null,
      "dtype": "float32",
      "description": "Scaling factor for routing weights."
    }
  },
  "outputs": {
    "output": {
      "shape": [
        "seq_len",
        "hidden_size"
      ],
      "dtype": "bfloat16",
      "description": "Final MoE output tensor"
    }
  },
  "reference": "import torch\r\n\r\n\r\n@torch.no_grad()\r\ndef run(\r\n    routing_logits: torch.Tensor,\r\n    routing_bias: torch.Tensor,\r\n    hidden_states: torch.Tensor,\r\n    hidden_states_scale: float,\r\n    gemm1_weights: torch.Tensor,\r\n    gemm1_scales: torch.Tensor,\r\n    gemm2_weights: torch.Tensor,\r\n    gemm2_scales: torch.Tensor,\r\n    local_expert_offset: int,\r\n    routed_scaling_factor: float,\r\n):\r\n    \"\"\"\r\n    FP8 per-tensor scale MoE with Renormalize routing (TopK -> Softmax).\r\n\r\n    • FP8 per-tensor dequantization (TRT-LLM convention): float = fp8 / scale\r\n      where scale = 448/amax, so effectively float = fp8 * (amax/448)\r\n    • Renormalize routing: TopK(logits + bias) → Softmax on top-k values\r\n    • Local computation:\r\n        only experts in [local_expert_offset, local_expert_offset + E_local) are\r\n        computed on this rank (GEMM1 → SwiGLU → GEMM2), then per-token weighted\r\n        accumulation.\r\n    \"\"\"\r\n\r\n    # Fixed geometry\r\n    H = 7168\r\n    I = 2048\r\n    E_local = gemm1_weights.shape[0]\r\n\r\n    E_global = routing_logits.shape[1]\r\n    T = routing_logits.shape[0]\r\n\r\n    assert H == 7168, \"hidden_size must be 7168\"\r\n    assert I == 2048, \"intermediate_size must be 2048\"\r\n    assert E_global == 256, \"num_experts must be 256\"\r\n    assert E_local == 32, \"num_local_experts must be 32\"\r\n\r\n    # Routing constants\r\n    TOP_K = 8\r\n\r\n    # Shape checks\r\n    assert hidden_states.shape == (T, H)\r\n    assert gemm1_weights.shape == (E_local, 2 * I, H)\r\n    assert gemm1_scales.shape == (E_local,)\r\n    assert gemm2_weights.shape == (E_local, H, I)\r\n    assert gemm2_scales.shape == (E_local,)\r\n    assert routing_bias.shape[-1] == E_global\r\n\r\n    device = hidden_states.device\r\n\r\n    # 1) FP8 per-tensor dequantization (TRT-LLM convention): float = fp8 / scale\r\n    # where scale = 448/amax, so dequant = fp8 * (amax/448)\r\n    # hidden_states: [T, H], scale: scalar\r\n    A = hidden_states.to(torch.float32) / hidden_states_scale  # [T, H] float32\r\n\r\n    # W13: [E_local, 2I, H], scale: [E_local]\r\n    W13_fp32 = gemm1_weights.to(torch.float32)\r\n    S13 = gemm1_scales.to(torch.float32)  # [E_local]\r\n    W13 = W13_fp32 / S13.view(E_local, 1, 1)  # [E, 2I, H] float32\r\n\r\n    # W2: [E_local, H, I], scale: [E_local]\r\n    W2_fp32 = gemm2_weights.to(torch.float32)\r\n    S2 = gemm2_scales.to(torch.float32)  # [E_local]\r\n    W2 = W2_fp32 / S2.view(E_local, 1, 1)  # [E, H, I] float32\r\n\r\n    # 2) Renormalize routing: TopK -> Softmax\r\n    logits = routing_logits.to(torch.float32)  # [T, E_global]\r\n    bias = routing_bias.to(torch.float32).reshape(-1)  # [E_global]\r\n\r\n    # First take top-k on raw logits (with bias added)\r\n    topk_logits, topk_idx = torch.topk(logits + bias, k=TOP_K, dim=1, largest=True, sorted=False)  # [T, K]\r\n\r\n    # Then apply softmax on top-k values only\r\n    topk_weights = torch.softmax(topk_logits, dim=-1)  # [T, K]\r\n\r\n    # Apply routing scaling factor\r\n    topk_weights = topk_weights * routed_scaling_factor  # [T, K]\r\n\r\n    # 3) Local expert compute and accumulation\r\n    output = torch.zeros((T, H), dtype=torch.float32, device=device)\r\n\r\n    local_start = int(local_expert_offset)\r\n\r\n    # For each local expert: find selected tokens, run GEMM1→SwiGLU→GEMM2, accumulate by weights\r\n    for le in range(E_local):\r\n        ge = local_start + le\r\n        if ge < 0 or ge >= E_global:\r\n            continue\r\n\r\n        # Find tokens that selected this global expert in their top-k\r\n        # topk_idx: [T, K] contains the expert indices\r\n        sel_mask_per_token = (topk_idx == ge).any(dim=1)  # [T] bool\r\n        if not sel_mask_per_token.any():\r\n            continue\r\n\r\n        token_idx = torch.nonzero(sel_mask_per_token, as_tuple=False).squeeze(1)  # [Tk]\r\n\r\n        # Gather inputs and weights for this expert\r\n        A_e = A.index_select(0, token_idx)  # [Tk, H]\r\n        W13_e = W13[le]  # [2I, H]\r\n        W2_e = W2[le]  # [H, I]\r\n\r\n        # GEMM1: [Tk, H] @ [H, 2I] = [Tk, 2I]\r\n        G1 = A_e.matmul(W13_e.t())  # [Tk, 2I]\r\n\r\n        # SwiGLU: split and apply silu(x) = x / (1 + exp(-x))\r\n        X1 = G1[:, :I]  # [Tk, I]\r\n        X2 = G1[:, I:]  # [Tk, I]\r\n        silu_X2 = X2 / (1.0 + torch.exp(-X2))  # [Tk, I]\r\n        C = silu_X2 * X1  # [Tk, I]\r\n\r\n        # GEMM2: [Tk, I] @ [I, H] = [Tk, H]\r\n        O = C.matmul(W2_e.t())  # [Tk, H]\r\n\r\n        # Get routing weights for this expert from topk_weights\r\n        # Find which position in top-k this expert is at for each token\r\n        topk_idx_tok = topk_idx.index_select(0, token_idx)  # [Tk, K]\r\n        topk_weights_tok = topk_weights.index_select(0, token_idx)  # [Tk, K]\r\n        expert_mask = (topk_idx_tok == ge)  # [Tk, K]\r\n        w_tok = (topk_weights_tok * expert_mask.float()).sum(dim=1)  # [Tk]\r\n\r\n        output.index_add_(0, token_idx, O * w_tok.unsqueeze(1))  # [Tk,H] * [Tk,1]\r\n\r\n    return output.to(torch.bfloat16)"
}
