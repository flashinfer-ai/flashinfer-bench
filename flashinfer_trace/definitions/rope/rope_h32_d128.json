{
  "name": "rope_h32_d128",
  "op_type": "rope",
  "description": "Rotary Position Embedding with 32 heads and head_dim=128. Used in Llama-3.1-8B and Qwen3 models.",
  "tags": [
    "status:verified",
    "model:llama-3.1-8b",
    "model:qwen3"
  ],
  "axes": {
    "batch_size": {
      "type": "var",
      "description": "Batch size"
    },
    "num_q_heads": {
      "type": "const",
      "value": 32
    },
    "num_k_heads": {
      "type": "var",
      "description": "Number of key heads (may differ from query heads in GQA)"
    },
    "seq_len": {
      "type": "var",
      "description": "Sequence length"
    },
    "head_dim": {
      "type": "const",
      "value": 128
    }
  },
  "inputs": {
    "q": {
      "shape": ["batch_size", "num_q_heads", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Query tensor"
    },
    "k": {
      "shape": ["batch_size", "num_k_heads", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Key tensor"
    },
    "cos": {
      "shape": ["batch_size", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Cosine positional encoding"
    },
    "sin": {
      "shape": ["batch_size", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Sine positional encoding"
    }
  },
  "outputs": {
    "q_embed": {
      "shape": ["batch_size", "num_q_heads", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Query with rotary embedding applied"
    },
    "k_embed": {
      "shape": ["batch_size", "num_k_heads", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Key with rotary embedding applied"
    }
  },
  "reference": "import torch\n\n@torch.no_grad()\ndef run(q, k, cos, sin):\n    batch_size, num_q_heads, seq_len, head_dim = q.shape\n    num_k_heads = k.shape[1]\n    \n    # Check constants\n    assert num_q_heads == 32\n    assert head_dim == 128\n    \n    def rotate_half(x):\n        x1 = x[..., : x.shape[-1] // 2]\n        x2 = x[..., x.shape[-1] // 2 :]\n        return torch.cat((-x2, x1), dim=-1)\n    \n    # Expand cos and sin for broadcasting: [batch, seq, dim] -> [batch, 1, seq, dim]\n    cos = cos.unsqueeze(1)\n    sin = sin.unsqueeze(1)\n    \n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    \n    return q_embed.to(q.dtype), k_embed.to(k.dtype)"
}
