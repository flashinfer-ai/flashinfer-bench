{
  "name": "rope_h64_d128",
  "op_type": "rope",
  "description": "Rotary Position Embedding with 64 heads and head_dim=128. Used in Llama-3.1-70B.",
  "tags": [
    "status:verified",
    "model:llama-3.1-70b"
  ],
  "axes": {
    "batch_size": {
      "type": "var",
      "description": "Batch size"
    },
    "num_q_heads": {
      "type": "const",
      "value": 64
    },
    "num_k_heads": {
      "type": "var",
      "description": "Number of key heads (may differ from query heads in GQA)"
    },
    "seq_len": {
      "type": "var",
      "description": "Sequence length"
    },
    "head_dim": {
      "type": "const",
      "value": 128
    }
  },
  "inputs": {
    "q": {
      "shape": ["batch_size", "num_q_heads", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Query tensor"
    },
    "k": {
      "shape": ["batch_size", "num_k_heads", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Key tensor"
    },
    "cos": {
      "shape": ["seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Cosine positional encoding"
    },
    "sin": {
      "shape": ["seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Sine positional encoding"
    }
  },
  "outputs": {
    "q_embed": {
      "shape": ["batch_size", "num_q_heads", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Query with rotary embedding applied"
    },
    "k_embed": {
      "shape": ["batch_size", "num_k_heads", "seq_len", "head_dim"],
      "dtype": "bfloat16",
      "description": "Key with rotary embedding applied"
    }
  },
  "reference": "import torch\n\n@torch.no_grad()\ndef run(q, k, cos, sin):\n    batch_size, num_q_heads, seq_len, head_dim = q.shape\n    num_k_heads = k.shape[1]\n    \n    # Check constants\n    assert num_q_heads == 64\n    assert head_dim == 128\n    \n    def rotate_half(x):\n        x1 = x[..., : x.shape[-1] // 2]\n        x2 = x[..., x.shape[-1] // 2 :]\n        return torch.cat((-x2, x1), dim=-1)\n    \n    # Expand cos and sin for broadcasting\n    cos = cos.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, head_dim]\n    sin = sin.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, head_dim]\n    \n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    \n    return q_embed.to(q.dtype), k_embed.to(k.dtype)"
}
