{
  "name": "gdn_decode_qk16_v32_d128_k_last",
  "description": "Gated Delta Net decode with GVA configuration and k-last state layout. Single-token generation with recurrent state update. Captured from Qwen3 Next linear attention layers.",
  "op_type": "gdn",
  "tags": [
    "stage:decode",
    "status:verified",
    "model:qwen3-next",
    "layout:k-last"
  ],
  "axes": {
    "batch_size": {
      "type": "var",
      "description": "Number of sequences being decoded concurrently."
    },
    "seq_len": {
      "type": "const",
      "value": 1,
      "description": "Sequence length (always 1 for single-token decode)."
    },
    "num_q_heads": {
      "type": "const",
      "value": 16,
      "description": "Number of query heads (same as key heads in GVA mode)."
    },
    "num_k_heads": {
      "type": "const",
      "value": 16,
      "description": "Number of key heads."
    },
    "num_v_heads": {
      "type": "const",
      "value": 32,
      "description": "Number of value heads (GVA: more value heads than query heads)."
    },
    "head_size": {
      "type": "const",
      "value": 128
    }
  },
  "constraints": [
    "num_v_heads >= num_q_heads",
    "num_v_heads % num_q_heads == 0",
    "num_k_heads == num_q_heads"
  ],
  "inputs": {
    "q": {
      "shape": [
        "batch_size",
        "seq_len",
        "num_q_heads",
        "head_size"
      ],
      "dtype": "bfloat16",
      "description": "Query tensor for single token decode."
    },
    "k": {
      "shape": [
        "batch_size",
        "seq_len",
        "num_k_heads",
        "head_size"
      ],
      "dtype": "bfloat16",
      "description": "Key tensor for single token decode."
    },
    "v": {
      "shape": [
        "batch_size",
        "seq_len",
        "num_v_heads",
        "head_size"
      ],
      "dtype": "bfloat16",
      "description": "Value tensor for single token decode."
    },
    "state": {
      "shape": [
        "batch_size",
        "num_v_heads",
        "head_size",
        "head_size"
      ],
      "dtype": "float32",
      "description": "Recurrent state in k-last layout [B, H, V, K]. Contains accumulated key-value outer products."
    },
    "A_log": {
      "shape": [
        "num_v_heads"
      ],
      "dtype": "float32",
      "description": "Log decay parameter. Per-head learnable parameter."
    },
    "a": {
      "shape": [
        "batch_size",
        "seq_len",
        "num_v_heads"
      ],
      "dtype": "bfloat16",
      "description": "Input-dependent decay. Combined with dt_bias via softplus."
    },
    "dt_bias": {
      "shape": [
        "num_v_heads"
      ],
      "dtype": "bfloat16",
      "description": "Decay bias. Per-head learnable parameter."
    },
    "b": {
      "shape": [
        "batch_size",
        "seq_len",
        "num_v_heads"
      ],
      "dtype": "bfloat16",
      "description": "Update gate input. Transformed via sigmoid to get beta."
    },
    "scale": {
      "shape": null,
      "dtype": "float32",
      "description": "Scale factor for query. Default is 1.0."
    }
  },
  "outputs": {
    "output": {
      "shape": [
        "batch_size",
        "seq_len",
        "num_v_heads",
        "head_size"
      ],
      "dtype": "bfloat16",
      "description": "Attention output. Shape follows num_v_heads in GVA mode."
    },
    "new_state": {
      "shape": [
        "batch_size",
        "num_v_heads",
        "head_size",
        "head_size"
      ],
      "dtype": "float32",
      "description": "Updated recurrent state in k-last layout [B, H, V, K]."
    }
  },
  "reference": "import torch\nimport torch.nn.functional as F\n\n\n@torch.no_grad()\ndef run(q, k, v, state, A_log, a, dt_bias, b, scale):\n    \"\"\"\n    Gated Delta Net decode reference implementation (k-last layout).\n    \n    State layout: [B, H, V, K] (k-last, V dimension before K dimension)\n    \n    Gating computation:\n        g = -exp(A_log) * softplus(a + dt_bias)\n        beta = sigmoid(b)\n    \n    Recurrent update (k-last layout):\n        state = state * exp(g)           # Apply decay\n        v_new = v - state @ k            # Prediction error\n        v_new = v_new * beta             # Apply update gate\n        state = state + v_new @ k^T      # Update state\n        output = scale * state @ q       # Compute output\n    \"\"\"\n    B, T, num_q_heads, K = q.shape\n    _, _, num_k_heads, _ = k.shape\n    _, _, num_v_heads, V = v.shape\n    num_heads = num_v_heads\n    device = q.device\n    \n    # Check constants\n    assert num_q_heads == 16\n    assert num_k_heads == 16\n    assert num_v_heads == 32\n    assert K == 128\n    assert V == 128\n    assert T == 1  # Decode is single token\n    \n    # Default scale\n    if scale is None or scale == 0.0:\n        scale = 1.0\n    \n    # Convert to float32 for computation\n    q_f32 = q.squeeze(1).float()  # [B, num_q_heads, K]\n    k_f32 = k.squeeze(1).float()  # [B, num_k_heads, K]\n    v_f32 = v.squeeze(1).float()  # [B, num_v_heads, V]\n    A_log_f32 = A_log.float()     # [num_heads]\n    a_f32 = a.squeeze(1).float()  # [B, num_heads]\n    dt_bias_f32 = dt_bias.float() # [num_heads]\n    b_f32 = b.squeeze(1).float()  # [B, num_heads]\n    state_f32 = state.float()     # [B, num_heads, V, K] (k-last)\n    \n    # Compute g = -exp(A_log) * softplus(a + dt_bias)\n    x = a_f32 + dt_bias_f32  # [B, num_heads]\n    softplus_x = F.softplus(x, beta=1.0, threshold=20.0)\n    g = -torch.exp(A_log_f32) * softplus_x  # [B, num_heads]\n    \n    # Compute beta = sigmoid(b)\n    beta = torch.sigmoid(b_f32)  # [B, num_heads]\n    \n    # Expand for GVA: q and k get repeated to match v heads\n    q_exp = q_f32.repeat_interleave(num_v_heads // num_q_heads, dim=1)  # [B, 32, K]\n    k_exp = k_f32.repeat_interleave(num_v_heads // num_k_heads, dim=1)  # [B, 32, K]\n    \n    # Apply L2 normalization (matching kernel behavior)\n    q_exp = F.normalize(q_exp, p=2.0, dim=-1)\n    k_exp = F.normalize(k_exp, p=2.0, dim=-1)\n    \n    # Apply scale\n    q_exp = q_exp * scale\n    \n    # Initialize outputs\n    new_state = torch.zeros_like(state_f32)\n    output = torch.zeros(B, num_heads, V, dtype=torch.float32, device=device)\n    \n    # Process each batch and head\n    for b_idx in range(B):\n        for h_idx in range(num_heads):\n            q_h = q_exp[b_idx, h_idx]       # [K]\n            k_h = k_exp[b_idx, h_idx]       # [K]\n            v_h = v_f32[b_idx, h_idx]       # [V]\n            h_state = state_f32[b_idx, h_idx].clone()  # [V, K] (k-last)\n            g_val = g[b_idx, h_idx]\n            beta_val = beta[b_idx, h_idx]\n            \n            # Step 1: Apply decay to state\n            h_state = h_state * torch.exp(g_val)\n            \n            # Step 2: Compute prediction error (delta rule)\n            # state @ k: [V, K] @ [K] = [V]\n            v_pred = h_state @ k_h\n            v_new = v_h - v_pred\n            \n            # Step 3: Apply update gate\n            v_new = v_new * beta_val\n            \n            # Step 4: Update state with outer product\n            # state += v_new @ k^T: [V, K] += [V, 1] @ [1, K]\n            h_state = h_state + v_new.unsqueeze(1) @ k_h.unsqueeze(0)\n            \n            # Step 5: Compute output\n            # state @ q: [V, K] @ [K] = [V]\n            output[b_idx, h_idx] = h_state @ q_h\n            \n            # Store updated state\n            new_state[b_idx, h_idx] = h_state\n    \n    # Reshape output to [B, 1, H, V]\n    output = output.unsqueeze(1).to(torch.bfloat16)\n    \n    return {\"output\": output, \"new_state\": new_state}"
}
