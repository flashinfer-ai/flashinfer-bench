{
  "name": "silu_h28672",
  "op_type": "silu",
  "description": "SiLU (Swish) activation with hidden_size=28672. Used in Llama-3.1-70B MLP (SwiGLU).",
  "tags": [
    "status:verified",
    "model:llama-3.1-70b"
  ],
  "axes": {
    "batch_size": {
      "type": "var",
      "description": "Total number of tokens"
    },
    "hidden_size": {
      "type": "const",
      "value": 28672
    }
  },
  "inputs": {
    "input": {
      "shape": ["batch_size", "hidden_size"],
      "dtype": "bfloat16",
      "description": "Input tensor"
    }
  },
  "outputs": {
    "output": {
      "shape": ["batch_size", "hidden_size"],
      "dtype": "bfloat16",
      "description": "Output after SiLU activation"
    }
  },
  "reference": "import torch\n\n@torch.no_grad()\ndef run(input):\n    batch_size, hidden_size = input.shape\n    \n    # Check constants\n    assert hidden_size == 28672\n    \n    # SiLU: x * sigmoid(x)\n    output = input * torch.sigmoid(input.float())\n    return output.to(input.dtype)"
}
