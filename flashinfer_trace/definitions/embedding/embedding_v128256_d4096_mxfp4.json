{
  "name": "embedding_v128256_d4096_mxfp4",
  "op_type": "embedding",
  "description": "Token embedding lookup with vocab_size=128256, embedding_dim=4096 using MXFP4 (Microscaling FP4) weights. For MXFP4-quantized models.",
  "tags": [
    "status:experimental",
    "model:llama-3.1-8b-mxfp4",
    "dtype:mxfp4"
  ],
  "axes": {
    "batch_size": {
      "type": "var",
      "description": "Number of input tokens"
    },
    "vocab_size": {
      "type": "const",
      "value": 128256
    },
    "embedding_dim": {
      "type": "const",
      "value": 4096
    }
  },
  "inputs": {
    "input_ids": {
      "shape": ["batch_size"],
      "dtype": "int64",
      "description": "Token indices to look up"
    },
    "weight": {
      "shape": ["vocab_size", "embedding_dim"],
      "dtype": "float4_e2m1",
      "description": "MXFP4 quantized embedding weight matrix"
    }
  },
  "outputs": {
    "output": {
      "shape": ["batch_size", "embedding_dim"],
      "dtype": "float4_e2m1",
      "description": "Embedded token representations in MXFP4"
    }
  },
  "reference": "import torch\n\n@torch.no_grad()\ndef run(input_ids, weight):\n    batch_size = input_ids.shape[0]\n    vocab_size, embedding_dim = weight.shape\n    \n    assert vocab_size == 128256\n    assert embedding_dim == 4096\n    \n    # Convert to float for indexing\n    weight_float = weight.to(torch.float16)\n    output = weight_float[input_ids]\n    return output.to(weight.dtype)"
}
