{
  "name": "embedding_v128256_d4096",
  "op_type": "embedding",
  "description": "Token embedding lookup with vocab_size=128256, embedding_dim=4096. Captured from Llama-3.1-8B.",
  "tags": [
    "status:verified",
    "model:llama-3.1-8b"
  ],
  "axes": {
    "batch_size": {
      "type": "var",
      "description": "Number of input tokens"
    },
    "vocab_size": {
      "type": "const",
      "value": 128256
    },
    "embedding_dim": {
      "type": "const",
      "value": 4096
    }
  },
  "inputs": {
    "input_ids": {
      "shape": ["batch_size"],
      "dtype": "int64",
      "description": "Token indices to look up"
    },
    "weight": {
      "shape": ["vocab_size", "embedding_dim"],
      "dtype": "bfloat16",
      "description": "Embedding weight matrix"
    }
  },
  "outputs": {
    "output": {
      "shape": ["batch_size", "embedding_dim"],
      "dtype": "bfloat16",
      "description": "Embedded token representations"
    }
  },
  "reference": "import torch\n\n@torch.no_grad()\ndef run(input_ids, weight):\n    batch_size = input_ids.shape[0]\n    vocab_size, embedding_dim = weight.shape\n    \n    # Check constants\n    assert vocab_size == 128256\n    assert embedding_dim == 4096\n    \n    output = weight[input_ids]\n    return output"
}
