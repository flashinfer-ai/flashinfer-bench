---
name: collect-workloads
description: Auto-collect workloads from SGLang inference runs using FlashInfer logging API. Dumps tensors, sanitizes them according to kernel definitions, and submits PR to flashinfer-trace workload repo.
---

# Collect Workloads

Automatically collect real-world workloads by running SGLang inference with FlashInfer Level 10 logging, then sanitize and submit to the flashinfer-ai/flashinfer-trace HuggingFace dataset repository.

## Description

This skill automates the complete workload collection pipeline:
1. **Update SGLang**: Get latest main branch from SGLang and install it
2. **Create branch**: Checkout new branch `collect_workload_for_{definition_name}` in SGLang
3. **Setup FlashInfer logging**: Set `FLASHINFER_LOGLEVEL=10`, `FLASHINFER_DUMP_DIR`, and `FLASHINFER_DUMP_INCLUDE` filter to only dump target APIs
4. **Launch SGLang server**: Start server with recommended config (or custom launch command)
5. **Run inference**: Use `sglang.bench_serving` with ShareGPT dataset
6. **Sanitize tensors**: Process dumped tensors and generate workload JSONL files (TODO)
7. **Submit PR**: Create pull request to flashinfer-ai/flashinfer-trace dataset repo (TODO)

## Usage

```bash
# Collect workloads for a specific definition
/collect-workloads --definition-name gqa_paged_decode_h32_kv8_d128_ps1

# Collect with custom model
/collect-workloads --definition-name mla_paged_decode_h16_ckv512_kpe64_ps1 --model-name deepseek-ai/DeepSeek-V3 --tp 4

# Collect with custom launch command
/collect-workloads --definition-name gdn_decode_qk16_v32_d128_k_last --launch-command "python3 -m sglang.launch_server --model Qwen/Qwen3-Next-80B-A3B-Instruct --tp 4"

# Collect with more prompts for better coverage
/collect-workloads --definition-name rmsnorm_h7168 --num-prompts 2000

# Custom SGLang path
/collect-workloads --definition-name gqa_paged_decode_h32_kv8_d128_ps1 --sglang-path /path/to/sglang
```

## Parameters

- `definition_name` (required): Definition name to collect workloads for (e.g., "gqa_paged_decode_h32_kv8_d128_ps1")
- `sglang_path` (optional): Path to SGLang repository (default: "./tmp/sglang")
- `conda_env` (optional): Conda environment name (default: "flashinfer")
- `model_name` (optional): Model to run inference on (auto-inferred from definition if not provided)
  - GQA kernels â†’ "meta-llama/Llama-3.1-8B-Instruct"
  - MLA/DSA kernels â†’ "deepseek-ai/DeepSeek-V3"
  - GDN kernels â†’ "Qwen/Qwen3-Next-80B-A3B-Instruct"
- `launch_command` (optional): Custom SGLang launch command (overrides default config)
  - Example: "python3 -m sglang.launch_server --model Qwen/Qwen3-Next-80B-A3B-Instruct --tp 4"
- `num_prompts` (optional): Number of prompts to process from ShareGPT dataset (default: 1000)
- `tp` (optional): Tensor parallelism degree (default: 1)

## Prerequisites

Run `/clone-repos` first to set up the `tmp/` directory with SGLang and FlashInfer (the `flashinfer_trace/` directory is already part of this repository).

## What This Skill Does

### Step 1: Update SGLang from Main Branch

1. **Fetch latest main branch**:
   ```bash
   cd $SGLANG_PATH
   git fetch origin
   git checkout main
   git pull origin main
   ```

2. **Install SGLang**:
   ```bash
   conda activate $CONDA_ENV
   pip install -e '.[all]'
   ```

3. **Verify installation**:
   ```bash
   python -c "import sglang; print(sglang.__version__)"
   ```

### Step 2: Create Workload Collection Branch

1. **Checkout new branch** in SGLang repository:
   ```bash
   cd $SGLANG_PATH
   git checkout -b collect_workload_for_{definition_name}
   ```

2. This branch isolates the workload collection work and can be used to:
   - Add instrumentation or debugging code if needed
   - Track which definitions have been collected
   - Keep collection work separate from main development

### Step 3: Setup FlashInfer Logging Environment with API Filtering

1. **Create dump directory**:
   ```bash
   mkdir -p $SGLANG_PATH/flashinfer_dumps
   ```

2. **Set environment variables** in the conda environment:
   ```bash
   conda activate $CONDA_ENV
   export FLASHINFER_LOGLEVEL=10
   export FLASHINFER_DUMP_DIR=$SGLANG_PATH/flashinfer_dumps
   export FLASHINFER_DUMP_SAFETENSORS=1
   export FLASHINFER_DUMP_MAX_COUNT=10000
   export FLASHINFER_DUMP_INCLUDE="*decode*"  # Filter pattern based on definition name
   ```

3. **Auto-generated filter patterns** (from definition tags):

   The filter is generated by reading `api:*` tags from the definition's `tags` field:

   **With api tags** (preferred):
   ```json
   "tags": ["api:batch_decode_with_paged_kv_cache"]
   â†’ FLASHINFER_DUMP_INCLUDE="*batch_decode_with_paged_kv_cache*"

   "tags": ["api:gdn_prefill"]
   â†’ FLASHINFER_DUMP_INCLUDE="*gdn_prefill*"

   "tags": ["api:rmsnorm", "api:fused_add_rmsnorm"]
   â†’ FLASHINFER_DUMP_INCLUDE="*rmsnorm*,*fused_add_rmsnorm*"
   ```

   **Fallback** (if no api tags present, inferred from definition name):
   - `gqa_paged_decode_h32_kv8_d128_ps1` â†’ `FLASHINFER_DUMP_INCLUDE="*gqa_paged*,*decode*"`
   - `mla_paged_prefill_h16_ckv512_kpe64_ps1` â†’ `FLASHINFER_DUMP_INCLUDE="*mla_paged*,*prefill*"`
   - `rmsnorm_h7168` â†’ `FLASHINFER_DUMP_INCLUDE="*rmsnorm*"`

4. **Environment variable reference**:
   - `FLASHINFER_LOGLEVEL=10`: Enable "Flight Recorder (Metadata + Tensors)" mode
   - `FLASHINFER_DUMP_DIR`: Directory where tensor dumps will be saved
   - `FLASHINFER_DUMP_SAFETENSORS=1`: Use safetensors format for dumps
   - `FLASHINFER_DUMP_MAX_COUNT`: Limit maximum number of dumps
   - `FLASHINFER_DUMP_INCLUDE`: Only dump APIs matching the filter pattern (reduces dump size significantly)

5. **Adding FlashInfer API tags to definitions**:

   To ensure accurate filtering, add `api:*` tags to definition JSON files:

   ```json
   {
     "name": "gqa_paged_decode_h32_kv8_d128_ps1",
     "tags": [
       "stage:decode",
       "status:verified",
       "model:llama-3.1-8b",
       "api:batch_decode_with_paged_kv_cache"
     ],
     ...
   }
   ```

   Common FlashInfer API names :
   - GQA: `batch_decode_with_paged_kv_cache`, `batch_prefill_with_paged_kv_cache`, `batch_prefill_with_ragged_kv_cache`
   - MLA: `mla_decode_with_paged_kv_cache`, `mla_prefill_with_paged_kv_cache`
   - DSA: `dsa_sparse_attention`, `dsa_topk_indexer`
   - GDN: `gdn_decode`, `gdn_prefill`
   - RMSNorm: `rmsnorm`, `fused_add_rmsnorm`
   - MoE: `moe_router`, `moe_scatter`, `fused_moe`

**Reference**:
- [FlashInfer Logging Documentation](https://docs.flashinfer.ai/logging.html)
- [FlashInfer PR #2206 - Tensor Dump & Replay](https://github.com/flashinfer-ai/flashinfer/pull/2206)

### Step 4: Launch SGLang Server

1. **Use recommended config** (if `--launch-command` not provided):

   Based on op_type and definition, use cookbook-recommended launch commands:

   - **GDN kernels** (Qwen3-Next):
     ```bash
     python3 -m sglang.launch_server \
       --model Qwen/Qwen3-Next-80B-A3B-Instruct \
       --tp 4 \
       --host 0.0.0.0 \
       --port 30000 \
       --attention-backend flashinfer \
       --disable-cuda-graph \
       --log-level info
     ```

   - **MLA/DSA kernels** (DeepSeek-V3):
     ```bash
     python3 -m sglang.launch_server \
       --model deepseek-ai/DeepSeek-V3 \
       --tp 8 \
       --host 0.0.0.0 \
       --port 30000 \
       --attention-backend flashinfer \
       --disable-cuda-graph \
       --log-level info
     ```

   - **GQA kernels** (Llama-3.1):
     ```bash
     python3 -m sglang.launch_server \
       --model meta-llama/Llama-3.1-8B-Instruct \
       --tp 1 \
       --host 0.0.0.0 \
       --port 30000 \
       --attention-backend flashinfer \
       --disable-cuda-graph \
       --log-level info
     ```

2. **Key flags**:
   - `--attention-backend flashinfer`: **Required** to use FlashInfer kernels
   - `--disable-cuda-graph`: **Required** to ensure every kernel call is logged individually
   - `--tp N`: Tensor parallelism based on model size and GPU availability

3. **Wait for server to be ready** (typically 30-60 seconds)

### Step 5: Run Inference Benchmark

1. **Use sglang.bench_serving** with ShareGPT dataset:
   ```bash
   python3 -m sglang.bench_serving \
     --backend sglang \
     --dataset-name sharegpt \
     --num-prompts 1000
   ```

2. **Parameters**:
   - `--backend sglang`: Use SGLang backend (connects to localhost:30000)
   - `--dataset-name sharegpt`: Use built-in ShareGPT dataset
   - `--num-prompts 1000`: Process 1000 prompts (adjustable based on coverage needs)

3. **During inference**:
   - FlashInfer will dump tensors to `$FLASHINFER_DUMP_DIR`
   - Each kernel call creates a dump with metadata and tensor data
   - Monitor disk space as dumps can be large (GBs)

4. **After completion**:
   - Shutdown the server gracefully
   - Dumps are ready for sanitization (Step 6)

**Important Notes**:
- Level 10 enables "Flight Recorder (Metadata + Tensors)" mode
- Dumps are saved in `.safetensors` format when `FLASHINFER_DUMP_SAFETENSORS=1`
- Each API call creates:
  - `session.jsonl`: Central log with all API events
  - Per-call directories with `metadata.jsonl`, `inputs.safetensors`, `outputs.safetensors`

### Phase 3: SGLang Inference Execution

1. **Launch SGLang Server** with FlashInfer backend:

   ```bash
   # Start SGLang server in background with model
   python -m sglang.launch_server \
     --model-path {model_path} \
     --host 0.0.0.0 \
     --port 30000 \
     --tp 1 \
     --attention-backend flashinfer \
     --disable-cuda-graph \
     --log-level info \
     &

   # Wait for server to be ready
   sleep 30
   ```

   **Key flags**:
   - `--attention-backend flashinfer`: Use FlashInfer kernels (enables logging)
   - `--disable-cuda-graph`: Ensures every kernel call is logged
   - `--tp 1`: Tensor parallelism (adjust based on GPU availability)

2. **Run Inference Requests**:

   ```python
   import requests
   import json
   from tqdm import tqdm

   # Load ShareGPT dataset
   with open(dataset_path) as f:
       conversations = [json.loads(line) for line in f][:num_samples]

   # Process each conversation
   for conv in tqdm(conversations):
       # Format as chat completion
       messages = conv["conversations"]

       # Send to SGLang server
       response = requests.post(
           "http://localhost:30000/v1/chat/completions",
           json={
               "model": model_name,
               "messages": messages,
               "max_tokens": 512,
               "temperature": 0.7,
           }
       )

       # Brief pause between requests
       time.sleep(0.1)
   ```

3. **Shutdown Server**:
   ```bash
   # Gracefully shutdown SGLang server
   pkill -f "sglang.launch_server"
   ```

### Phase 4: Tensor Dump Processing

1. **Locate Dump Directory**:
   ```bash
   DUMP_DIR=$(ls -td workload_dumps_* | head -1)
   echo "Processing dumps from: $DUMP_DIR"
   ```

2. **Parse Session Log**:
   - Read `session.jsonl` to understand API call sequence
   - Extract metadata for each call: function name, timestamp, call ID
   - Map to kernel definitions based on function names

3. **Load Tensor Dumps**:
   ```python
   from safetensors import safe_open

   # For each API call directory
   call_dirs = sorted(Path(DUMP_DIR).glob("call_*"))

   for call_dir in call_dirs:
       # Load metadata
       with open(call_dir / "metadata.jsonl") as f:
           metadata = json.loads(f.readline())

       # Load input tensors
       inputs = {}
       with safe_open(call_dir / "inputs.safetensors", framework="pt") as f:
           for key in f.keys():
               inputs[key] = f.get_tensor(key)

       # Identify which definition this matches
       definition_name = match_to_definition(metadata["function_name"], inputs)
   ```

### Phase 5: Workload Sanitization

**Map raw tensor dumps to workload specifications**:

1. **Match to Definition Schema**:
   - Load definition JSON for target kernel
   - Extract axes (constant and variable)
   - Extract input/output specifications

2. **Extract Variable Axes**:
   ```python
   def extract_axes(definition, tensors):
       """Extract variable axis values from actual tensor shapes."""
       axes = {}

       # Parse definition axes
       for axis_name, axis_spec in definition["axes"].items():
           if axis_spec["type"] == "const":
               # Verify constant matches expected value
               continue
           elif axis_spec["type"] == "var":
               # Extract from tensor shapes
               axes[axis_name] = infer_axis_value(tensors, axis_name, definition)

       return axes
   ```

3. **Handle Tensor Storage**:

   **For small tensors (< 1MB)**: Use `"type": "random"` for reproducibility

   **For large tensors or unique patterns**: Save to safetensors
   ```python
   def create_workload_entry(definition_name, tensors, definition):
       """Create a workload JSONL entry."""
       # Extract axes
       axes = extract_axes(definition, tensors)

       # Prepare inputs
       inputs = {}
       for input_name, input_spec in definition["inputs"].items():
           tensor = tensors[input_name]

           # Decision: random vs saved tensor
           if tensor.numel() < 262144:  # < 1MB for fp16
               inputs[input_name] = {"type": "random"}
           else:
               # Save to safetensors
               safetensor_path = f"tensors/{definition_name}/{uuid}.safetensors"
               save_file({input_name: tensor}, safetensor_path)
               inputs[input_name] = {
                   "type": "safetensors",
                   "path": safetensor_path,
                   "tensor_key": input_name
               }

       # Create workload
       return {
           "definition": definition_name,
           "solution": None,
           "workload": {
               "uuid": str(uuid.uuid4()),
               "axes": axes,
               "inputs": inputs
           },
           "evaluation": None
       }
   ```

4. **Generate Workload JSONL**:
   ```python
   # Group by definition
   workloads_by_def = defaultdict(list)

   for call_dir in processed_calls:
       entry = create_workload_entry(...)
       workloads_by_def[entry["definition"]].append(entry)

   # Write to JSONL files
   for def_name, entries in workloads_by_def.items():
       op_type = load_definition(def_name)["op_type"]
       output_path = f"flashinfer_trace/workloads/{op_type}/{def_name}.jsonl"

       # Append to existing or create new
       with open(output_path, "a") as f:
           for entry in entries:
               f.write(json.dumps(entry) + "\n")
   ```

5. **Deduplication**:
   - Check for duplicate workloads (same axes + input patterns)
   - Only keep unique configurations
   - Prioritize diversity in batch_size, seq_len, and other variable axes

### Phase 6: Submit Pull Request

1. **Clone flashinfer-trace Repository**:
   ```bash
   # Clone HF dataset repo (requires git-lfs)
   cd tmp/
   git clone https://huggingface.co/datasets/flashinfer-ai/flashinfer-trace
   cd flashinfer-trace
   ```

2. **Create Branch**:
   ```bash
   # Generate branch name from definitions
   BRANCH_NAME="workloads-$(date +%Y%m%d)-${op_type:-mixed}"
   git checkout -b $BRANCH_NAME
   ```

3. **Copy Workload Files**:
   ```bash
   # Copy new/updated workload JSONL files
   for def_name in $COLLECTED_DEFINITIONS; do
       op_type=$(get_op_type $def_name)
       cp ../../flashinfer-bench/flashinfer_trace/workloads/$op_type/$def_name.jsonl \
          workloads/$op_type/$def_name.jsonl
   done

   # Copy any new tensor safetensors files
   if [ -d "../../flashinfer-bench/flashinfer_trace/workloads/tensors/" ]; then
       cp -r ../../flashinfer-bench/flashinfer_trace/workloads/tensors/* \
             workloads/tensors/
   fi
   ```

4. **Commit Changes**:
   ```bash
   # Stage all workload changes
   git add workloads/

   # Create commit
   git commit -m "Add workloads for ${op_type:-multiple op_types}

   Collected from ${model_name} inference run with ${num_samples} samples.

   Definitions covered:
   $(echo "$COLLECTED_DEFINITIONS" | sed 's/^/- /')

   Stats:
   - Total workloads: ${total_count}
   - Op types: ${op_types_list}
   - Source model: ${model_name}
   - Dataset: ShareGPT (${num_samples} samples)

   Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
   ```

5. **Push and Create PR**:
   ```bash
   # Push branch
   git push origin $BRANCH_NAME

   # Create PR using Hugging Face CLI (if available) or GitHub CLI
   # Note: HuggingFace datasets use GitHub backend
   gh pr create \
     --repo flashinfer-ai/flashinfer-trace \
     --title "Add ${op_type:-mixed} workloads from ${model_name}" \
     --body "$(cat <<EOF
   ## Summary

   This PR adds real-world workloads collected from ${model_name} inference runs.

   ### Collection Details
   - **Model**: ${model_name}
   - **Dataset**: ShareGPT (${num_samples} samples)
   - **Definitions**: ${#COLLECTED_DEFINITIONS[@]} definitions across ${#op_types[@]} op_types
   - **Total Workloads**: ${total_count}

   ### Definitions Covered
   $(echo "$COLLECTED_DEFINITIONS" | sed 's/^/- `/' | sed 's/$/`/')

   ### Op Types
   $(echo "$op_types_list" | sed 's/^/- /')

   ### Workload Statistics
   \`\`\`
   $(for def in $COLLECTED_DEFINITIONS; do
       echo "$def: $(wc -l < workloads/$op_type/$def.jsonl) workloads"
   done)
   \`\`\`

   ### Validation
   - [x] All workloads follow JSONL format
   - [x] Axes match definition schemas
   - [x] Input specifications are valid
   - [x] Deduplication applied
   - [x] Tensor files saved (if applicable)

   ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
   EOF
   )"
   ```

## Output Format

### Console Output

During execution, the script prints:

```
======================================================================
FlashInfer Workload Collection: gqa_paged_decode_h32_kv8_d128_ps1
======================================================================
Definition: gqa_paged_decode_h32_kv8_d128_ps1
Op Type: gqa_paged
SGLang Path: /Users/averyhuang/Documents/FLASHINFER/flashinfer-bench/tmp/sglang
Conda Env: flashinfer
Dump Dir: /Users/averyhuang/Documents/FLASHINFER/flashinfer-bench/tmp/sglang/flashinfer_dumps
======================================================================

============================================================
Step 1: Update SGLang from main branch
============================================================
Fetching latest changes from main...
$ git fetch origin
$ git checkout main
$ git pull origin main
Installing SGLang...
$ pip install -e '.[all]'
âœ“ SGLang installed: version 0.4.2

============================================================
Step 2: Create workload collection branch
============================================================
Creating new branch 'collect_workload_for_gqa_paged_decode_h32_kv8_d128_ps1'...
$ git checkout -b collect_workload_for_gqa_paged_decode_h32_kv8_d128_ps1
âœ“ On branch: collect_workload_for_gqa_paged_decode_h32_kv8_d128_ps1

============================================================
Step 3: Configure FlashInfer logging
============================================================
âœ“ FLASHINFER_LOGLEVEL=10
âœ“ FLASHINFER_DUMP_DIR=/path/to/sglang/flashinfer_dumps
âœ“ FLASHINFER_DUMP_SAFETENSORS=1
âœ“ FLASHINFER_DUMP_MAX_COUNT=10000
âœ“ FLASHINFER_DUMP_INCLUDE=*gqa_paged*,*decode*
  (Only dumps APIs matching: *gqa_paged*,*decode*)

============================================================
Step 4: Launch SGLang server
============================================================
Using default launch command:
  python3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct --tp 1 --host 0.0.0.0 --port 30000 --attention-backend flashinfer --disable-cuda-graph --log-level info
Starting SGLang server (this may take a few minutes)...
Waiting for server to be ready...
  Waiting for server... (1/10)
  Waiting for server... (2/10)
âœ“ SGLang server is ready

============================================================
Step 5: Run inference benchmark
============================================================
Running benchmark with 1000 prompts...
  python3 -m sglang.bench_serving --backend sglang --dataset-name sharegpt --num-prompts 1000
âœ“ Benchmark complete

Benchmark output:
[Benchmark statistics here...]

============================================================
TODO: Steps 6+ (sanitization, workload generation, PR submission)
============================================================
Raw dumps available at: /path/to/sglang/flashinfer_dumps

============================================================
Cleanup: Stopping SGLang server
============================================================
âœ“ Server stopped gracefully

======================================================================
Workload Collection Complete (Steps 1-5)
======================================================================
Next steps:
  1. Review dumps in: /path/to/sglang/flashinfer_dumps
  2. Implement sanitization (Step 6)
  3. Generate workload JSONL
  4. Submit PR to flashinfer-trace
```

### Generated Files

**Workload JSONL files** (appended to existing):
```
flashinfer_trace/workloads/
â”œâ”€â”€ mla_paged/
â”‚   â”œâ”€â”€ mla_paged_decode_h16_ckv512_kpe64_ps1.jsonl  (78 new entries)
â”‚   â””â”€â”€ mla_paged_prefill_h16_ckv512_kpe64_ps1.jsonl
â”œâ”€â”€ rmsnorm/
â”‚   â”œâ”€â”€ rmsnorm_h7168.jsonl  (102 new entries)
â”‚   â””â”€â”€ fused_add_rmsnorm_h7168.jsonl
â””â”€â”€ moe/
    â””â”€â”€ moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048.jsonl  (54 new entries)
```

**Safetensors files** (if large tensors saved):
```
flashinfer_trace/workloads/tensors/
â”œâ”€â”€ mla_paged_decode_h16_ckv512_kpe64_ps1/
â”‚   â”œâ”€â”€ a1b2c3d4-e5f6-7890-abcd-ef1234567890.safetensors
â”‚   â””â”€â”€ ...
â””â”€â”€ moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048/
    â””â”€â”€ ...
```

## Implementation

When Claude invokes this skill, it will:

1. **Execute the Python script** [`collect_workloads.py`](./collect_workloads.py):
   ```bash
   python .claude/skills/collect-workloads/collect_workloads.py \
     --definition-name {definition_name} \
     --sglang-path {sglang_path} \
     --conda-env {conda_env} \
     --model-name {model_name} \
     --num-prompts {num_prompts} \
     --tp {tp}
   ```

2. **The script orchestrates all 5 steps**:
   - Step 1: Update SGLang from main branch
   - Step 2: Create branch `collect_workload_for_{definition_name}`
   - Step 3: Set FlashInfer logging environment with `FLASHINFER_DUMP_INCLUDE` filter
   - Step 4: Launch SGLang server
   - Step 5: Run `sglang.bench_serving` with ShareGPT

3. **Output**:
   - Raw tensor dumps in `$SGLANG_PATH/flashinfer_dumps/` (only matching APIs)
   - Server logs and benchmark results
   - Ready for sanitization (Step 6, TODO)

## ShareGPT Dataset Format

The ShareGPT dataset uses conversation format:

```json
{
  "id": "conversation_id",
  "conversations": [
    {"from": "human", "value": "User message"},
    {"from": "gpt", "value": "Assistant response"},
    ...
  ]
}
```

**Alternative datasets**:
- Alpaca format
- OpenAI chat format
- Custom JSONL with similar structure

## Kernel Name Mapping

Map FlashInfer API function names to definitions:

| FlashInfer API | Definition Op Type | Example Definition |
|---------------|-------------------|-------------------|
| `batch_decode_with_paged_kv_cache` | `gqa_paged` | `gqa_paged_decode_h32_kv8_d128_ps1` |
| `batch_prefill_with_paged_kv_cache` | `gqa_paged` | `gqa_paged_prefill_causal_h32_kv8_d128_ps1` |
| `mla_decode_with_paged_kv_cache` | `mla_paged` | `mla_paged_decode_h16_ckv512_kpe64_ps1` |
| `rmsnorm` | `rmsnorm` | `rmsnorm_h7168` |
| `fused_add_rmsnorm` | `rmsnorm` | `fused_add_rmsnorm_h7168` |
| `moe_*` | `moe` | `moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048` |
| `gemm` / `linear` | `gemm` | `gemm_n6144_k4096` |

## Error Handling

### SGLang Server Fails to Start
- **Error**: Model loading failure or OOM
- **Handling**:
  - Check GPU memory availability
  - Reduce `--tp` (tensor parallelism) if needed
  - Try smaller model variant

### No Tensor Dumps Generated
- **Error**: Dump directory empty after inference
- **Handling**:
  - Verify `FLASHINFER_LOGLEVEL=10` is set
  - Check `FLASHINFER_DUMP_INCLUDE` filters aren't excluding target kernels
  - Ensure `--attention-backend flashinfer` flag is used
  - Check SGLang is actually using FlashInfer (not falling back to other backends)

### Definition Mismatch
- **Error**: Tensor shapes don't match definition schema
- **Handling**:
  - Print detailed shape comparison
  - Check if model uses different tensor parallel configuration
  - Flag for manual review (may need new definition variant)

### PR Submission Fails
- **Error**: Authentication or push failure
- **Handling**:
  - Verify Hugging Face authentication: `huggingface-cli login`
  - Check write permissions to flashinfer-ai/flashinfer-trace
  - Provide local workload file paths for manual PR creation

## Integration with Other Skills

```bash
# Complete workflow: Extract definitions â†’ Collect workloads â†’ Add tests

# 1. Clone repos
/clone-repos

# 2. Extract kernel definitions from model
/extract-kernel-definitions --model-name deepseek_v3

# 3. Collect workloads for new definitions
/collect-workloads --op-type mla_paged --model-name deepseek-v3

# 4. Add reference tests
/add-reference-tests --op-type mla_paged

# 5. Run tests with real workloads
pytest flashinfer_trace/tests/references/test_mla_paged*.py -v
```

## Advanced Usage

### Collecting for Multiple Models

```bash
# Collect from different models for better workload diversity
/collect-workloads --op-type gqa_paged --model-name llama-3.1-8b --num-samples 100
/collect-workloads --op-type gqa_paged --model-name qwen2.5-7b --num-samples 100
/collect-workloads --op-type gqa_paged --model-name mistral-7b --num-samples 100
```

### Custom Filtering with FLASHINFER_DUMP_INCLUDE

```bash
# Only dump specific kernel types
export FLASHINFER_DUMP_INCLUDE="*decode*,*rmsnorm*"

# Exclude certain kernels
export FLASHINFER_DUMP_EXCLUDE="*prefill*"
```

### Replay and Validation

After collecting, validate workloads using FlashInfer replay:

```bash
# Replay a dump directory to verify correctness
flashinfer replay --dir workload_dumps_20260202_143022 --compare-outputs
```

## Notes

- FlashInfer Level 10 logging can generate **large amounts of data** (GBs per inference run)
  - Use `FLASHINFER_DUMP_MAX_SIZE_GB` and `FLASHINFER_DUMP_MAX_COUNT` to limit
  - Apply `FLASHINFER_DUMP_INCLUDE` filters to target specific kernels
- Deduplication is **critical** to avoid bloating the dataset with redundant workloads
- Prefer `"type": "random"` for small tensors (faster benchmarking, reproducible)
- Use `"type": "safetensors"` only for large tensors or when patterns are important
- SGLang's `--disable-cuda-graph` flag is **required** to ensure all kernels are logged individually
- The flashinfer-trace repo may require approval for contributions (contact @flashinfer-ai team)

## Troubleshooting

### Issue: Workloads have all random inputs

**Cause**: Tensors were small enough to be marked as random

**Solution**: This is expected behavior; random inputs provide good test coverage while keeping dataset size manageable

### Issue: Variable axes don't match definition

**Cause**: Tensor parallel or model variant differences

**Solution**:
1. Check definition's constant axes match model config
2. May need to create new definition variant for this TP configuration
3. Use `/extract-kernel-definitions` to generate proper definition

### Issue: PR rejected due to dataset quality

**Cause**: Workloads may be too synthetic or redundant

**Solution**:
- Increase `num_samples` for more diversity
- Use different datasets (not just ShareGPT)
- Collect from multiple models
- Ensure deduplication is working properly

## Maintaining This Document

Update this file when:
- FlashInfer logging API changes (new environment variables, file formats)
- Workload schema changes (new InputSpec types)
- HuggingFace dataset repo structure changes
- SGLang server launch parameters change

## References

- [FlashInfer Logging Documentation](https://docs.flashinfer.ai/logging.html)
- [FlashInfer PR #2311 (API Decorator Refactoring)](https://github.com/flashinfer-ai/flashinfer/pull/2311)
- [FlashInfer PR #2206 (Tensor Dump & Replay)](https://github.com/flashinfer-ai/flashinfer/pull/2206)
- [flashinfer-ai/flashinfer-trace Dataset](https://huggingface.co/datasets/flashinfer-ai/flashinfer-trace)
- [SGLang Documentation](https://sgl-project.github.io/)
- [ShareGPT Dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)

## See Also

- [clone-repos](../clone-repos/SKILL.md)
- [extract-kernel-definitions](../extract-kernel-definitions/SKILL.md)
- [add-reference-tests](../add-reference-tests/SKILL.md)
