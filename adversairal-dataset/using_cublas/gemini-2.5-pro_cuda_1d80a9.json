{
  "name": "gemini-2.5-pro_cuda_1d80a9",
  "definition": "gemm_n4096_k4096",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.cpp::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "kernel.h",
      "content": "#pragma once\n\n// Include guard\n#ifndef GEMM_N4096_K4096_KERNEL_H\n#define GEMM_N4096_K4096_KERNEL_H\n\n#include <cuda_fp16.h>\n#include <cuda_runtime.h>\n\n/**\n * @brief Host-side launcher for the GEMM operation C = A * B.T.\n *\n * This function launches a cuBLAS-based GEMM kernel optimized for\n * B200-class GPUs with N=4096 and K=4096. It computes C = A * B.T\n * where A is [M, K], B is [N, K], and C is [M, N]. All tensors are\n * expected to have row-major layout.\n *\n * @param M The variable dimension of matrix A and C.\n * @param A Pointer to the input matrix A [M, 4096] in device memory (dtype=half).\n * @param B Pointer to the input matrix B [4096, 4096] in device memory (dtype=half).\n * @param C Pointer to the output matrix C [M, 4096] in device memory (dtype=half).\n * @param stream The CUDA stream for kernel execution.\n */\nvoid gemm_n4096_k4096_launcher(\n    int M,\n    const half* A,\n    const half* B,\n    half* C,\n    cudaStream_t stream\n);\n\n#endif // GEMM_N4096_K4096_KERNEL_H"
    },
    {
      "path": "kernel.cu",
      "content": "#include \"kernel.h\"\n#include <cublas_v2.h>\n#include <cuda_fp16.h>\n\n#include <stdexcept>\n#include <string>\n\n// Helper for checking cuBLAS status and converting it to a descriptive error\n#define CUBLAS_CHECK(status)                                         \\\n  do {                                                               \\\n    cublasStatus_t err = (status);                                   \\\n    if (err != CUBLAS_STATUS_SUCCESS) {                              \\\n      throw std::runtime_error(\"cuBLAS error: \" + std::to_string(err) \\\n                               + \" at \" __FILE__ \":\" + std::to_string(__LINE__)); \\\n    }                                                                \\\n  } while (0)\n\n// Manages a singleton cuBLAS handle for efficiency.\n// This avoids the overhead of creating/destroying the handle on every call.\ncublasHandle_t get_cublas_handle() {\n    static struct CublasHandle {\n        cublasHandle_t handle;\n        CublasHandle() {\n            CUBLAS_CHECK(cublasCreate(&handle));\n            // Enable Tensor Core operations for peak performance on B200\n            CUBLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));\n        }\n        ~CublasHandle() {\n            if (handle) cublasDestroy(handle);\n        }\n    } singleton_handle;\n    return singleton_handle.handle;\n}\n\n/**\n * @brief Host-side launcher implementation using cuBLAS.\n */\nvoid gemm_n4096_k4096_launcher(\n    int M,\n    const half* A,\n    const half* B,\n    half* C,\n    cudaStream_t stream\n) {\n    cublasHandle_t handle = get_cublas_handle();\n    CUBLAS_CHECK(cublasSetStream(handle, stream));\n\n    const int N = 4096;\n    const int K = 4096;\n\n    const float alpha = 1.0f;\n    const float beta = 0.0f;\n\n    // The key to using cuBLAS (column-major) with row-major PyTorch tensors is\n    // to rephrase the operation in a way that cuBLAS understands and that results\n    // in the correct memory layout for the output.\n    //\n    // 1. Goal (Row-Major): C_rm[M, N] = A_rm[M, K] * B_rm.T[K, N]\n    //\n    // 2. cuBLAS View (Column-Major): cuBLAS interprets the memory of a row-major\n    //    matrix X_rm[rows, cols] as a column-major matrix X_cm[cols, rows].\n    //    - A_rm[M, K] is seen as A_cm[K, M].\n    //    - B_rm[N, K] is seen as B_cm[K, N].\n    //    - C_rm[M, N] is seen as C_cm[N, M].\n    //\n    // 3. Transformation: The equation C_rm = A_rm * B_rm.T is equivalent to\n    //    C_cm.T = A_cm.T * (B_cm.T).T => C_cm.T = A_cm.T * B_cm.\n    //    Taking the transpose of the whole equation gives us what cuBLAS should compute:\n    //    C_cm = (A_cm.T * B_cm).T = B_cm.T * A_cm.\n    //\n    // 4. cuBLAS Call: We ask cuBLAS to compute D = op1 * op2, where the result D\n    //    is written into the memory of C.\n    //    - op1 = B_cm.T. This means the first matrix is B, and transa=CUBLAS_OP_T.\n    //    - op2 = A_cm.   This means the second matrix is A, and transb=CUBLAS_OP_N.\n    //\n    // 5. Dimensions for cuBLAS:\n    //    - m = rows of op1 (B.T) = N\n    //    - n = cols of op2 (A)   = M\n    //    - k = common dimension  = K\n    //    The output matrix will be [m, n] = [N, M] in column-major layout, which\n    //    perfectly matches the memory layout of our desired row-major C_rm[M, N].\n    //    This resolves the illegal memory access and ensures correctness.\n    const int lda = K; // Leading dimension of A_rm[M, K] is K\n    const int ldb = K; // Leading dimension of B_rm[N, K] is K\n    const int ldc = N; // Leading dimension of C_rm[M, N] is N\n\n    CUBLAS_CHECK(cublasGemmEx(\n        handle,\n        CUBLAS_OP_T,        // transa: Corresponds to first matrix (B), transposed\n        CUBLAS_OP_N,        // transb: Corresponds to second matrix (A), not transposed\n        N,                  // m: rows of op(B.T)\n        M,                  // n: columns of op(A)\n        K,                  // k: common dimension\n        &alpha,             // alpha\n        B,                  // Pointer to the first matrix (B)\n        CUDA_R_16F,         // Btype\n        ldb,                // ldb (leading dimension of B)\n        A,                  // Pointer to the second matrix (A)\n        CUDA_R_16F,         // Atype\n        lda,                // lda (leading dimension of A)\n        &beta,              // beta\n        C,                  // Pointer to C\n        CUDA_R_16F,         // Ctype\n        ldc,                // ldc (leading dimension of C)\n        CUDA_R_32F,         // computeType: Use FP32 accumulators for precision\n        CUBLAS_GEMM_DEFAULT_TENSOR_OP // algorithm: Use default heuristic for Tensor Cores\n    ));\n}"
    },
    {
      "path": "main.cpp",
      "content": "#include <torch/extension.h>\n#include <c10/cuda/CUDAStream.h>\n\n#include \"kernel.h\"\n\n#include <stdexcept>\n#include <string>\n\n// Helper macros for concise tensor validation\n#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_HALF(x) TORCH_CHECK(x.scalar_type() == torch::kFloat16, #x \" must be a float16 tensor\")\n\n/**\n * @brief PyTorch extension entry point for the GEMM operation.\n *\n * This function validates input tensors and calls the CUDA kernel launcher\n * to perform the computation C = A * B.T on the GPU.\n *\n * @param A A torch::Tensor of shape [M, 4096] and dtype float16.\n * @param B A torch::Tensor of shape [4096, 4096] and dtype float16.\n * @return A torch::Tensor C of shape [M, 4096] and dtype float16 containing the result.\n */\ntorch::Tensor run(torch::Tensor A, torch::Tensor B) {\n    // --- Input Validation ---\n    CHECK_CUDA(A);\n    CHECK_CUDA(B);\n    CHECK_CONTIGUOUS(A);\n    CHECK_CONTIGUOUS(B);\n    CHECK_HALF(A);\n    CHECK_HALF(B);\n\n    TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n    TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n\n    // --- Dimension Checks ---\n    const int M = A.size(0);\n    const int K_A = A.size(1);\n    const int N_B = B.size(0);\n    const int K_B = B.size(1);\n\n    const int N_spec = 4096;\n    const int K_spec = 4096;\n\n    TORCH_CHECK(K_A == K_spec, \"A must have shape [M, 4096], but K is \", K_A);\n    TORCH_CHECK(N_B == N_spec, \"B must have shape [4096, 4096], but N is \", N_B);\n    TORCH_CHECK(K_B == K_spec, \"B must have shape [4096, 4096], but K is \", K_B);\n    TORCH_CHECK(A.device() == B.device(), \"Tensors must be on the same CUDA device\");\n\n    // --- Output Tensor Allocation ---\n    auto C_options = torch::TensorOptions()\n        .device(A.device())\n        .dtype(A.scalar_type());\n    auto C = torch::empty({M, N_spec}, C_options);\n\n    // --- Kernel Execution ---\n    try {\n        // Get the current CUDA stream from PyTorch's context to ensure proper synchronization\n        cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n        // Get raw data pointers. at::Half is compatible with cuda_fp16.h::half\n        const half* A_ptr = reinterpret_cast<const half*>(A.data_ptr<at::Half>());\n        const half* B_ptr = reinterpret_cast<const half*>(B.data_ptr<at::Half>());\n        half* C_ptr = reinterpret_cast<half*>(C.data_ptr<at::Half>());\n\n        // Launch the cuBLAS-based kernel\n        gemm_n4096_k4096_launcher(M, A_ptr, B_ptr, C_ptr, stream);\n\n    } catch (const std::exception& e) {\n        // Propagate exceptions from the CUDA/cuBLAS calls to Python\n        throw std::runtime_error(std::string(\"CUDA kernel execution failed: \") + e.what());\n    }\n\n    // Check for any asynchronous errors from the kernel launch. This is good practice.\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(std::string(\"CUDA asynchronous error: \") + cudaGetErrorString(err));\n    }\n\n    return C;\n}\n\n// Pybind11 module definition to expose the 'run' function to Python\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"run\", &run, \"GEMM (A * B.T) for N=4096, K=4096 using a B200-optimized cuBLAS kernel\");\n}"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for gemm_n4096_k4096 (round 8)"
}
