{
  "name": "gpt-5-2025-08-07_cuda_a2d8ca",
  "definition": "moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048",
  "author": "gpt-5-2025-08-07",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.cpp::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "kernel.h",
      "content": "#ifndef MOE_FP8_BLOCK_SCALE_DS_ROUTING_TOPK8_NG8_KG4_E32_H7168_I2048_KERNEL_H_\n#define MOE_FP8_BLOCK_SCALE_DS_ROUTING_TOPK8_NG8_KG4_E32_H7168_I2048_KERNEL_H_\n\n#include <cuda_runtime.h>\n#include <cstdint>\n\n// B200-tuned constants for this specialized kernel\nstatic constexpr int HIDDEN_SIZE = 7168;        // H\nstatic constexpr int INTERMEDIATE_SIZE = 2048;  // I\nstatic constexpr int GEMM1_OUT_SIZE = 4096;     // 2 * I\nstatic constexpr int NUM_EXPERTS_GLOBAL = 256;  // E_global\nstatic constexpr int NUM_LOCAL_EXPERTS = 32;    // E_local\nstatic constexpr int BLOCK_SIZE_128 = 128;\n\nstatic constexpr int NUM_HIDDEN_BLOCKS = 56;         // H / 128\nstatic constexpr int NUM_INTERMEDIATE_BLOCKS = 16;   // I / 128\nstatic constexpr int NUM_GEMM1_OUT_BLOCKS = 32;      // (2*I)/128\n\n// DeepSeek routing constants\nstatic constexpr int ROUTE_TOP_K = 8;\nstatic constexpr int ROUTE_NUM_GROUP = 8;\nstatic constexpr int ROUTE_GROUP_SIZE = 32;    // NUM_EXPERTS_GLOBAL / ROUTE_NUM_GROUP\nstatic constexpr int ROUTE_TOPK_GROUP = 4;\n\n// Error check macro\n#define CUDA_CHECK(status) \\\n  do { \\\n    cudaError_t err__ = (status); \\\n    if (err__ != cudaSuccess) { \\\n      fprintf(stderr, \"CUDA Error %s at %s:%d\\n\", cudaGetErrorString(err__), __FILE__, __LINE__); \\\n    } \\\n  } while (0)\n\n// Kernel launchers\n\n// 1) No-aux routing with group-top2 and global top-k=8\nvoid launch_noaux_routing_topk8(\n    const float* routing_logits,   // [T, 256]\n    const float* routing_bias,     // [256] (float32)\n    int T,                         // seq_len\n    float routed_scaling_factor,\n    int* __restrict__ topk_idx,    // [T, 8] (int32)\n    float* __restrict__ topk_w,    // [T, 8] (float32)\n    cudaStream_t stream);\n\n// 2) Hidden states block-scale application (after FP8 -> float32 conversion)\nvoid launch_apply_hidden_block_scale(\n    float* __restrict__ A_fp32,     // [T, H], in-place\n    const float* __restrict__ hs_scale, // [H/128, T] contiguous\n    int T,\n    cudaStream_t stream);\n\n// 3) Apply 128x128 block scale to 2D matrix (in-place)\nvoid launch_apply_block_scale_128x128(\n    float* __restrict__ M,          // [rows, cols], row-major\n    int rows,                       // multiple of 128\n    int cols,                       // multiple of 128\n    const float* __restrict__ S,    // [rows/128, cols/128], row-major\n    int S_rows,                     // rows/128\n    int S_cols,                     // cols/128\n    cudaStream_t stream);\n\n// 4) Count assignments per local expert\nvoid launch_count_local_assignments(\n    const int* __restrict__ topk_idx,  // [T, 8]\n    int T,\n    int local_expert_offset,\n    int* __restrict__ counts,          // [32], zero-initialized\n    cudaStream_t stream);\n\n// 5) Fill flat assignment lists using prefix offsets (atomic on-device)\nvoid launch_fill_local_assignments(\n    const int* __restrict__ topk_idx,   // [T, 8]\n    const float* __restrict__ topk_w,   // [T, 8]\n    int T,\n    int local_expert_offset,\n    int* __restrict__ offsets_inout,    // [32], device-side running offsets (initialized with prefix \"offsets\")\n    int* __restrict__ token_ids_out,    // [total_assignments]\n    float* __restrict__ token_w_out,    // [total_assignments]\n    cudaStream_t stream);\n\n// 6) Gather rows from [T, H] by token_ids to a compact [Tk, H]\nvoid launch_gather_rows(\n    const float* __restrict__ A,      // [T, H]\n    const int* __restrict__ token_ids,// [Tk]\n    int T, int Tk, int H,\n    float* __restrict__ A_out,        // [Tk, H]\n    cudaStream_t stream);\n\n// 7) SwiGLU on GEMM1 output: C = silu(G1[:, I:]) * G1[:, :I]\nvoid launch_swiglu(\n    const float* __restrict__ G1,   // [Tk, 4096]\n    int Tk,\n    float* __restrict__ C,          // [Tk, 2048]\n    cudaStream_t stream);\n\n// 8) Accumulate O[Tk,H] into output[T,H] by token_ids and weights (no atomics if sequential per expert)\nvoid launch_accumulate_weighted_add(\n    const float* __restrict__ O,        // [Tk, H]\n    const int* __restrict__ token_ids,  // [Tk]\n    const float* __restrict__ weights,  // [Tk]\n    int Tk, int H,\n    float* __restrict__ output,         // [T, H]\n    cudaStream_t stream);\n\n#endif // MOE_FP8_BLOCK_SCALE_DS_ROUTING_TOPK8_NG8_KG4_E32_H7168_I2048_KERNEL_H_"
    },
    {
      "path": "kernel.cu",
      "content": "#include \"kernel.h\"\n#include <cstdio>\n#include <cmath>\n\n#ifndef CUDART_INF_F\n#define CUDART_INF_F (__int_as_float(0x7f800000))\n#endif\n\n// Warp reduce max\n__device__ __forceinline__ float warp_max(float v) {\n  #pragma unroll\n  for (int offset = 16; offset > 0; offset >>= 1) {\n    float other = __shfl_down_sync(0xffffffffu, v, offset);\n    v = fmaxf(v, other);\n  }\n  return v;\n}\n\n// 1) No-aux routing kernel\n// One block per token (T), 8 warps per block (256 threads), one warp per group\n__global__ void noaux_routing_topk8_kernel(\n    const float* __restrict__ logits,   // [T, 256]\n    const float* __restrict__ bias,     // [256]\n    int T,\n    float routed_scaling_factor,\n    int* __restrict__ topk_idx,         // [T, 8]\n    float* __restrict__ topk_w) {       // [T, 8]\n\n  __shared__ float group_scores[ROUTE_NUM_GROUP]; // 8\n  __shared__ unsigned int keep_group_mask;        // bitmask of 8 groups\n  __shared__ float warpCandVal[ROUTE_NUM_GROUP * ROUTE_TOP_K];        // 8*8 = 64\n  __shared__ int   warpCandIdx[ROUTE_NUM_GROUP * ROUTE_TOP_K];\n  __shared__ float warpCandSNoBias[ROUTE_NUM_GROUP * ROUTE_TOP_K];\n\n  int t = blockIdx.x;\n  if (t >= T) return;\n\n  const int lane = threadIdx.x & 31;\n  const int warp = threadIdx.x >> 5; // 0..7\n  const int e = warp * ROUTE_GROUP_SIZE + lane;  // expert index 0..255\n\n  // Load and compute s and s_with_bias\n  float l = logits[t * NUM_EXPERTS_GLOBAL + e];\n  float s = 1.f / (1.f + __expf(-l));\n  float sb = s + bias[e];\n\n  // Compute group top-2 sum within warp\n  float v = sb;\n  float m1 = warp_max(v);\n  unsigned mask1 = __ballot_sync(0xffffffffu, v == m1);\n  int idx1_lane = __ffs(mask1) - 1;\n  float v2 = (lane == idx1_lane) ? -CUDART_INF_F : v;\n  float m2 = warp_max(v2);\n  if (lane == 0) {\n    group_scores[warp] = m1 + m2;\n  }\n  __syncthreads();\n\n  // Select top-4 groups on a single thread\n  if (threadIdx.x == 0) {\n    float temp_scores[ROUTE_NUM_GROUP];\n    #pragma unroll\n    for (int g = 0; g < ROUTE_NUM_GROUP; ++g) temp_scores[g] = group_scores[g];\n    unsigned int mask_bits = 0u;\n    #pragma unroll\n    for (int j = 0; j < ROUTE_TOPK_GROUP; ++j) {\n      int best = 0;\n      float bestv = temp_scores[0];\n      #pragma unroll\n      for (int g = 1; g < ROUTE_NUM_GROUP; ++g) {\n        if (temp_scores[g] > bestv) { bestv = temp_scores[g]; best = g; }\n      }\n      mask_bits |= (1u << best);\n      temp_scores[best] = -CUDART_INF_F;\n    }\n    keep_group_mask = mask_bits;\n  }\n  __syncthreads();\n\n  // Prune unkept groups by setting -inf, keep sb for kept groups\n  bool keep = ((keep_group_mask >> warp) & 1u) != 0u;\n  float cur = keep ? sb : -CUDART_INF_F;\n\n  // Compute top-8 within this warp (group)\n  #pragma unroll\n  for (int j = 0; j < ROUTE_TOP_K; ++j) {\n    float m = warp_max(cur);\n    unsigned msk = __ballot_sync(0xffffffffu, cur == m);\n    int max_lane = __ffs(msk) - 1;\n    float s_no_bias_sel = __shfl_sync(0xffffffffu, s, max_lane);\n    if (lane == 0) {\n      int base = warp * ROUTE_TOP_K + j;\n      warpCandVal[base] = m;\n      warpCandIdx[base] = warp * ROUTE_GROUP_SIZE + max_lane;\n      warpCandSNoBias[base] = s_no_bias_sel;\n    }\n    if (lane == max_lane) cur = -CUDART_INF_F;\n  }\n  __syncthreads();\n\n  // Merge 64 candidates to top-8 globally\n  if (threadIdx.x == 0) {\n    float temp_val[ROUTE_NUM_GROUP * ROUTE_TOP_K];\n    int   temp_idx[ROUTE_NUM_GROUP * ROUTE_TOP_K];\n    float temp_snb[ROUTE_NUM_GROUP * ROUTE_TOP_K];\n\n    #pragma unroll\n    for (int i = 0; i < ROUTE_NUM_GROUP * ROUTE_TOP_K; ++i) {\n      temp_val[i] = warpCandVal[i];\n      temp_idx[i] = warpCandIdx[i];\n      temp_snb[i] = warpCandSNoBias[i];\n    }\n\n    float sel_s[ROUTE_TOP_K];\n    int sel_idx[ROUTE_TOP_K];\n\n    #pragma unroll\n    for (int j = 0; j < ROUTE_TOP_K; ++j) {\n      int best_i = 0;\n      float best_v = temp_val[0];\n      #pragma unroll\n      for (int i = 1; i < ROUTE_NUM_GROUP * ROUTE_TOP_K; ++i) {\n        if (temp_val[i] > best_v) { best_v = temp_val[i]; best_i = i; }\n      }\n      sel_idx[j] = temp_idx[best_i];\n      sel_s[j] = temp_snb[best_i];\n      temp_val[best_i] = -CUDART_INF_F;\n    }\n\n    // Normalize weights using s (no bias)\n    float sumw = 0.f;\n    #pragma unroll\n    for (int j = 0; j < ROUTE_TOP_K; ++j) sumw += sel_s[j];\n    sumw = fmaxf(sumw, 1e-20f);\n    #pragma unroll\n    for (int j = 0; j < ROUTE_TOP_K; ++j) {\n      float w = (sel_s[j] / sumw) * routed_scaling_factor;\n      topk_idx[t * ROUTE_TOP_K + j] = sel_idx[j];\n      topk_w[t * ROUTE_TOP_K + j] = w;\n    }\n  }\n}\n\n// 2) Hidden block scale application (in-place)\n__global__ void apply_hidden_block_scale_kernel(\n    float* __restrict__ A,            // [T, H]\n    const float* __restrict__ S,      // [H/128, T] in row-major\n    int T, int H) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int N = T * H;\n  for (int i = idx; i < N; i += blockDim.x * gridDim.x) {\n    int t = i / H;\n    int h = i - t * H;\n    int hb = h >> 7; // h/128\n    float sc = S[hb * T + t];\n    A[i] *= sc;\n  }\n}\n\n// 3) Apply 128x128 block scale to 2D matrix (in-place)\n__global__ void apply_block_scale_128x128_kernel(\n    float* __restrict__ M,     // [rows, cols]\n    int rows, int cols,\n    const float* __restrict__ S,// [rows/128, cols/128]\n    int Sb_rows, int Sb_cols) {\n\n  int blk_row = blockIdx.y; // 0..rows/128 - 1\n  int blk_col = blockIdx.x; // 0..cols/128 - 1\n  float scale = S[blk_row * Sb_cols + blk_col];\n\n  int row_base = blk_row * BLOCK_SIZE_128;\n  int col_base = blk_col * BLOCK_SIZE_128;\n\n  int tx = threadIdx.x; // 0..31\n  int ty = threadIdx.y; // 0..7\n\n  // Fully cover the 128x128 tile using 32x8 threads\n  for (int r = ty; r < BLOCK_SIZE_128; r += blockDim.y) {\n    int row = row_base + r;\n    float* row_ptr = M + row * cols;\n    for (int c = tx; c < BLOCK_SIZE_128; c += blockDim.x) {\n      int col = col_base + c;\n      row_ptr[col] *= scale;\n    }\n  }\n}\n\n// 4) Count assignments per local expert\n__global__ void count_local_assignments_kernel(\n    const int* __restrict__ topk_idx,   // [T, 8]\n    int T,\n    int local_expert_offset,\n    int* __restrict__ counts) {         // [32]\n  int t = blockIdx.x * blockDim.x + threadIdx.x;\n  if (t >= T) return;\n  int base = t * ROUTE_TOP_K;\n  #pragma unroll\n  for (int k = 0; k < ROUTE_TOP_K; ++k) {\n    int ge = topk_idx[base + k];\n    int le = ge - local_expert_offset;\n    if ((unsigned)le < (unsigned)NUM_LOCAL_EXPERTS) {\n      atomicAdd(&counts[le], 1);\n    }\n  }\n}\n\n// 5) Fill assignments using prefix offsets\n__global__ void fill_local_assignments_kernel(\n    const int* __restrict__ topk_idx,   // [T, 8]\n    const float* __restrict__ topk_w,   // [T, 8]\n    int T,\n    int local_expert_offset,\n    int* __restrict__ offsets_inout,    // [32], running counters\n    int* __restrict__ token_ids_out,    // [total]\n    float* __restrict__ token_w_out) {  // [total]\n  int t = blockIdx.x * blockDim.x + threadIdx.x;\n  if (t >= T) return;\n  int base = t * ROUTE_TOP_K;\n  #pragma unroll\n  for (int k = 0; k < ROUTE_TOP_K; ++k) {\n    int ge = topk_idx[base + k];\n    int le = ge - local_expert_offset;\n    if ((unsigned)le < (unsigned)NUM_LOCAL_EXPERTS) {\n      int pos = atomicAdd(&offsets_inout[le], 1);\n      token_ids_out[pos] = t;\n      token_w_out[pos] = topk_w[base + k];\n    }\n  }\n}\n\n// 6) Gather rows [T,H] -> [Tk,H]\n__global__ void gather_rows_kernel(\n    const float* __restrict__ A,     // [T, H]\n    const int* __restrict__ token_ids,// [Tk]\n    int /*T*/, int Tk, int H,\n    float* __restrict__ A_out) {     // [Tk, H]\n  int row = blockIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row >= Tk || col >= H) return;\n  int t = token_ids[row];\n  A_out[row * H + col] = A[t * H + col];\n}\n\n// 7) SwiGLU kernel\n__global__ void swiglu_kernel(\n    const float* __restrict__ G1, // [Tk, 4096]\n    int Tk,\n    float* __restrict__ C) {      // [Tk, 2048]\n  int row = blockIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row >= Tk || col >= INTERMEDIATE_SIZE) return;\n  const float* g1_row = G1 + row * GEMM1_OUT_SIZE;\n  float x1 = g1_row[col];\n  float x2 = g1_row[col + INTERMEDIATE_SIZE];\n  float silu = x2 / (1.0f + __expf(-x2));\n  C[row * INTERMEDIATE_SIZE + col] = silu * x1;\n}\n\n// 8) Accumulate O into output with weights\n__global__ void accumulate_weighted_add_kernel(\n    const float* __restrict__ O,       // [Tk, H]\n    const int* __restrict__ token_ids, // [Tk]\n    const float* __restrict__ weights, // [Tk]\n    int Tk, int H,\n    float* __restrict__ output) {      // [T, H]\n  int row = blockIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row >= Tk || col >= H) return;\n  int t = token_ids[row];\n  float w = weights[row];\n  float val = O[row * H + col] * w;\n  output[t * H + col] += val;\n}\n\n// Launchers\n\nvoid launch_noaux_routing_topk8(\n    const float* routing_logits,\n    const float* routing_bias,\n    int T,\n    float routed_scaling_factor,\n    int* topk_idx,\n    float* topk_w,\n    cudaStream_t stream) {\n\n  dim3 block(ROUTE_NUM_GROUP * 32); // 8 warps\n  dim3 grid(T);\n  noaux_routing_topk8_kernel<<<grid, block, 0, stream>>>(\n      routing_logits, routing_bias, T, routed_scaling_factor, topk_idx, topk_w);\n  CUDA_CHECK(cudaGetLastError());\n}\n\nvoid launch_apply_hidden_block_scale(\n    float* A_fp32,\n    const float* hs_scale,\n    int T,\n    cudaStream_t stream) {\n  int H = HIDDEN_SIZE;\n  int64_t N64 = static_cast<int64_t>(T) * H;\n  int threads = 256;\n  int blocks = static_cast<int>((N64 + threads - 1) / threads);\n  blocks = max(1, min(blocks, 65535));\n  apply_hidden_block_scale_kernel<<<blocks, threads, 0, stream>>>(A_fp32, hs_scale, T, H);\n  CUDA_CHECK(cudaGetLastError());\n}\n\nvoid launch_apply_block_scale_128x128(\n    float* M, int rows, int cols,\n    const float* S, int S_rows, int S_cols,\n    cudaStream_t stream) {\n\n  dim3 grid(S_cols, S_rows);   // blocks in [cols/128, rows/128]\n  dim3 block(32, 8);           // 256 threads\n  apply_block_scale_128x128_kernel<<<grid, block, 0, stream>>>(M, rows, cols, S, S_rows, S_cols);\n  CUDA_CHECK(cudaGetLastError());\n}\n\nvoid launch_count_local_assignments(\n    const int* topk_idx, int T, int local_expert_offset,\n    int* counts, cudaStream_t stream) {\n  int threads = 256;\n  int blocks = (T + threads - 1) / threads;\n  count_local_assignments_kernel<<<blocks, threads, 0, stream>>>(\n      topk_idx, T, local_expert_offset, counts);\n  CUDA_CHECK(cudaGetLastError());\n}\n\nvoid launch_fill_local_assignments(\n    const int* topk_idx, const float* topk_w, int T, int local_expert_offset,\n    int* offsets_inout, int* token_ids_out, float* token_w_out,\n    cudaStream_t stream) {\n  int threads = 256;\n  int blocks = (T + threads - 1) / threads;\n  fill_local_assignments_kernel<<<blocks, threads, 0, stream>>>(\n      topk_idx, topk_w, T, local_expert_offset, offsets_inout, token_ids_out, token_w_out);\n  CUDA_CHECK(cudaGetLastError());\n}\n\nvoid launch_gather_rows(\n    const float* A, const int* token_ids, int /*T*/, int Tk, int H,\n    float* A_out, cudaStream_t stream) {\n  dim3 block(256);\n  dim3 grid((H + block.x - 1) / block.x, Tk);\n  if (Tk > 0) {\n    gather_rows_kernel<<<grid, block, 0, stream>>>(A, token_ids, 0, Tk, H, A_out);\n    CUDA_CHECK(cudaGetLastError());\n  }\n}\n\nvoid launch_swiglu(\n    const float* G1, int Tk, float* C, cudaStream_t stream) {\n  dim3 block(256);\n  dim3 grid((INTERMEDIATE_SIZE + block.x - 1) / block.x, Tk);\n  if (Tk > 0) {\n    swiglu_kernel<<<grid, block, 0, stream>>>(G1, Tk, C);\n    CUDA_CHECK(cudaGetLastError());\n  }\n}\n\nvoid launch_accumulate_weighted_add(\n    const float* O, const int* token_ids, const float* weights, int Tk, int H,\n    float* output, cudaStream_t stream) {\n  dim3 block(256);\n  dim3 grid((H + block.x - 1) / block.x, Tk);\n  if (Tk > 0) {\n    accumulate_weighted_add_kernel<<<grid, block, 0, stream>>>(\n        O, token_ids, weights, Tk, H, output);\n    CUDA_CHECK(cudaGetLastError());\n  }\n}"
    },
    {
      "path": "main.cpp",
      "content": "#include \"kernel.h\"\n\n#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <c10/cuda/CUDAGuard.h>\n#include <cublas_v2.h>\n#include <vector>\n#include <algorithm>\n#include <stdexcept>\n#include <cstring>\n\n#define CUBLAS_CHECK(status) \\\n  do { \\\n    cublasStatus_t st__ = (status); \\\n    if (st__ != CUBLAS_STATUS_SUCCESS) { \\\n      fprintf(stderr, \"cuBLAS Error %d at %s:%d\\n\", int(st__), __FILE__, __LINE__); \\\n    } \\\n  } while (0)\n\nstatic inline void check_input(const torch::Tensor& t, c10::ScalarType dtype, const std::vector<int64_t>& shape_prefix) {\n  TORCH_CHECK(t.is_cuda(), \"Tensor must be CUDA\");\n  TORCH_CHECK(t.scalar_type() == dtype, \"Unexpected dtype\");\n  TORCH_CHECK(t.dim() >= (int)shape_prefix.size(), \"Unexpected rank\");\n  for (size_t i = 0; i < shape_prefix.size(); ++i) {\n    if (shape_prefix[i] >= 0) {\n      TORCH_CHECK(t.size(i) == shape_prefix[i], \"Unexpected size at dim \", i);\n    }\n  }\n}\n\ntorch::Tensor run(\n    torch::Tensor routing_logits,        // [T, 256], float32\n    torch::Tensor routing_bias,          // [256], bfloat16 (all zeros for no bias)\n    torch::Tensor hidden_states,         // [T, 7168], float8_e4m3fn\n    torch::Tensor hidden_states_scale,   // [56, T], float32\n    torch::Tensor gemm1_weights,         // [32, 4096, 7168], float8_e4m3fn\n    torch::Tensor gemm1_weights_scale,   // [32, 32, 56], float32\n    torch::Tensor gemm2_weights,         // [32, 7168, 2048], float8_e4m3fn\n    torch::Tensor gemm2_weights_scale,   // [32, 56, 16], float32\n    int64_t local_expert_offset,         // int\n    double routed_scaling_factor         // float\n) {\n  TORCH_CHECK(routing_logits.is_cuda(), \"routing_logits must be CUDA\");\n  TORCH_CHECK(hidden_states.is_cuda(), \"hidden_states must be CUDA\");\n  TORCH_CHECK(hidden_states_scale.is_cuda(), \"hidden_states_scale must be CUDA\");\n  TORCH_CHECK(gemm1_weights.is_cuda() && gemm1_weights_scale.is_cuda(), \"gemm1 weights must be CUDA\");\n  TORCH_CHECK(gemm2_weights.is_cuda() && gemm2_weights_scale.is_cuda(), \"gemm2 weights must be CUDA\");\n  TORCH_CHECK(routing_bias.is_cuda(), \"routing_bias must be CUDA\");\n\n  const int64_t T = routing_logits.size(0);\n  TORCH_CHECK(routing_logits.size(1) == NUM_EXPERTS_GLOBAL, \"routing_logits shape mismatch\");\n  TORCH_CHECK(hidden_states.size(0) == T && hidden_states.size(1) == HIDDEN_SIZE, \"hidden_states shape mismatch\");\n  TORCH_CHECK(hidden_states_scale.size(0) == NUM_HIDDEN_BLOCKS && hidden_states_scale.size(1) == T, \"hidden_states_scale shape mismatch\");\n  TORCH_CHECK(gemm1_weights.size(0) == NUM_LOCAL_EXPERTS &&\n              gemm1_weights.size(1) == GEMM1_OUT_SIZE &&\n              gemm1_weights.size(2) == HIDDEN_SIZE, \"gemm1_weights shape mismatch\");\n  TORCH_CHECK(gemm1_weights_scale.sizes() == torch::IntArrayRef({NUM_LOCAL_EXPERTS, NUM_GEMM1_OUT_BLOCKS, NUM_HIDDEN_BLOCKS}), \"gemm1_weights_scale shape mismatch\");\n  TORCH_CHECK(gemm2_weights.sizes() == torch::IntArrayRef({NUM_LOCAL_EXPERTS, HIDDEN_SIZE, INTERMEDIATE_SIZE}), \"gemm2_weights shape mismatch\");\n  TORCH_CHECK(gemm2_weights_scale.sizes() == torch::IntArrayRef({NUM_LOCAL_EXPERTS, NUM_HIDDEN_BLOCKS, NUM_INTERMEDIATE_BLOCKS}), \"gemm2_weights_scale shape mismatch\");\n  TORCH_CHECK(routing_bias.size(0) == NUM_EXPERTS_GLOBAL, \"routing_bias size mismatch\");\n\n  c10::cuda::CUDAGuard device_guard(routing_logits.device());\n  cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n  // Cast routing bias to float32 (device)\n  auto routing_bias_f32 = routing_bias.to(torch::kFloat32).contiguous();\n  auto routing_logits_f32 = routing_logits.contiguous(); // already float32\n\n  // 1) Hidden states FP8 -> float32 using PyTorch conversion, then apply block scale\n  auto A_fp32 = hidden_states.to(torch::kFloat32).contiguous();\n  TORCH_CHECK(A_fp32.size(0) == T && A_fp32.size(1) == HIDDEN_SIZE, \"A_fp32 shape mismatch\");\n  auto hs_scale_c = hidden_states_scale.contiguous();\n  launch_apply_hidden_block_scale(\n      A_fp32.data_ptr<float>(),\n      hs_scale_c.data_ptr<float>(),\n      (int)T, stream);\n\n  // 2) Routing: compute topk indices and weights\n  auto topk_idx = torch::empty({T, ROUTE_TOP_K}, torch::dtype(torch::kInt32).device(routing_logits.device()));\n  auto topk_w = torch::empty({T, ROUTE_TOP_K}, torch::dtype(torch::kFloat32).device(routing_logits.device()));\n  launch_noaux_routing_topk8(\n      routing_logits_f32.data_ptr<float>(),\n      routing_bias_f32.data_ptr<float>(),\n      (int)T,\n      static_cast<float>(routed_scaling_factor),\n      topk_idx.data_ptr<int>(),\n      topk_w.data_ptr<float>(),\n      stream);\n\n  // 3) Build local assignments for experts in [local_expert_offset, local_expert_offset + 32)\n  auto counts = torch::zeros({NUM_LOCAL_EXPERTS}, torch::dtype(torch::kInt32).device(routing_logits.device()));\n  launch_count_local_assignments(\n      topk_idx.data_ptr<int>(),\n      (int)T,\n      (int)local_expert_offset,\n      counts.data_ptr<int>(),\n      stream);\n\n  // Sync to read counts on host\n  CUDA_CHECK(cudaStreamSynchronize(stream));\n  auto counts_cpu = counts.cpu();\n  auto counts_ptr = counts_cpu.data_ptr<int>();\n  std::vector<int> h_counts(NUM_LOCAL_EXPERTS);\n  int total_assign = 0;\n  int max_Tk = 0;\n  for (int i = 0; i < NUM_LOCAL_EXPERTS; ++i) {\n    h_counts[i] = counts_ptr[i];\n    total_assign += h_counts[i];\n    max_Tk = std::max(max_Tk, h_counts[i]);\n  }\n  std::vector<int> h_offsets(NUM_LOCAL_EXPERTS + 1, 0);\n  for (int i = 0; i < NUM_LOCAL_EXPERTS; ++i) h_offsets[i + 1] = h_offsets[i] + h_counts[i];\n\n  // Allocate assignment buffers and fill\n  auto d_offsets = torch::empty({NUM_LOCAL_EXPERTS}, torch::dtype(torch::kInt32).device(routing_logits.device()));\n  CUDA_CHECK(cudaMemcpyAsync(d_offsets.data_ptr<int>(), h_offsets.data(), sizeof(int) * NUM_LOCAL_EXPERTS, cudaMemcpyHostToDevice, stream));\n  auto token_ids = torch::empty({std::max(1, total_assign)}, torch::dtype(torch::kInt32).device(routing_logits.device()));\n  auto token_wts = torch::empty({std::max(1, total_assign)}, torch::dtype(torch::kFloat32).device(routing_logits.device()));\n  launch_fill_local_assignments(\n      topk_idx.data_ptr<int>(),\n      topk_w.data_ptr<float>(),\n      (int)T,\n      (int)local_expert_offset,\n      d_offsets.data_ptr<int>(),\n      token_ids.data_ptr<int>(),\n      token_wts.data_ptr<float>(),\n      stream);\n\n  // 4) Output buffer (float32 accumulation)\n  auto output_f32 = torch::zeros({T, HIDDEN_SIZE}, torch::dtype(torch::kFloat32).device(routing_logits.device()));\n\n  // 5) cuBLAS handle\n  cublasHandle_t handle = nullptr;\n  CUBLAS_CHECK(cublasCreate(&handle));\n  CUBLAS_CHECK(cublasSetStream(handle, stream));\n\n  // 6) Per-expert processing\n  // Workspace sized by max_Tk\n  int Tk_max = std::max(1, max_Tk);\n  auto A_tok = torch::empty({Tk_max, HIDDEN_SIZE}, torch::dtype(torch::kFloat32).device(routing_logits.device()));\n  auto G1 = torch::empty({Tk_max, GEMM1_OUT_SIZE}, torch::dtype(torch::kFloat32).device(routing_logits.device()));\n  auto C = torch::empty({Tk_max, INTERMEDIATE_SIZE}, torch::dtype(torch::kFloat32).device(routing_logits.device()));\n  auto Otmp = torch::empty({Tk_max, HIDDEN_SIZE}, torch::dtype(torch::kFloat32).device(routing_logits.device()));\n\n  const float alpha = 1.0f, beta0 = 0.0f;\n\n  for (int le = 0; le < NUM_LOCAL_EXPERTS; ++le) {\n    int Tk = h_counts[le];\n    if (Tk == 0) continue;\n\n    int start = h_offsets[le];\n    const int* d_token_ids_le = token_ids.data_ptr<int>() + start;\n    const float* d_token_w_le = token_wts.data_ptr<float>() + start;\n\n    // Gather A_tok [Tk, H]\n    launch_gather_rows(\n        A_fp32.data_ptr<float>(),\n        d_token_ids_le,\n        (int)T, (int)Tk, HIDDEN_SIZE,\n        A_tok.data_ptr<float>(),\n        stream);\n\n    // Dequantize W13 for this local expert: take slice [le, :, :]\n    auto w13_fp8 = gemm1_weights.select(0, le).contiguous(); // [4096, 7168] float8\n    auto w13_f32 = w13_fp8.to(torch::kFloat32).contiguous(); // decode fp8 -> float32\n    auto s13 = gemm1_weights_scale.select(0, le).contiguous(); // [32, 56] float32\n    // Apply 128x128 block scale\n    launch_apply_block_scale_128x128(\n        w13_f32.data_ptr<float>(),\n        GEMM1_OUT_SIZE, HIDDEN_SIZE,\n        s13.data_ptr<float>(),\n        NUM_GEMM1_OUT_BLOCKS, NUM_HIDDEN_BLOCKS,\n        stream);\n\n    // GEMM1: G1[Tk, 4096] = A_tok[Tk, 7168] @ W13^T [7168, 4096]\n    // Column-major trick: C_cm(4096 x Tk) = (W13_cm^T)(4096x7168) * (A_cm)(7168xTk)\n    CUBLAS_CHECK(cublasSgemm(\n        handle,\n        CUBLAS_OP_T, CUBLAS_OP_N,\n        GEMM1_OUT_SIZE,        // m = 4096\n        Tk,                    // n = Tk\n        HIDDEN_SIZE,           // k = 7168\n        &alpha,\n        w13_f32.data_ptr<float>(), HIDDEN_SIZE,    // A: (7168 x 4096), lda=7168\n        A_tok.data_ptr<float>(), HIDDEN_SIZE,      // B: (7168 x Tk),  ldb=7168\n        &beta0,\n        G1.data_ptr<float>(),  GEMM1_OUT_SIZE));   // C: (4096 x Tk),  ldc=4096\n\n    // SwiGLU: C = silu(G1[:, I:]) * G1[:, :I]\n    launch_swiglu(G1.data_ptr<float>(), Tk, C.data_ptr<float>(), stream);\n\n    // Dequantize W2 for this expert: [7168, 2048] row-major\n    auto w2_fp8 = gemm2_weights.select(0, le).contiguous();   // [7168, 2048], fp8\n    auto w2_f32 = w2_fp8.to(torch::kFloat32).contiguous();    // [7168, 2048], row-major\n    auto s2 = gemm2_weights_scale.select(0, le).contiguous(); // [56, 16]\n    launch_apply_block_scale_128x128(\n        w2_f32.data_ptr<float>(),\n        HIDDEN_SIZE, INTERMEDIATE_SIZE,\n        s2.data_ptr<float>(),\n        NUM_HIDDEN_BLOCKS, NUM_INTERMEDIATE_BLOCKS,\n        stream);\n\n    // GEMM2: Otmp[Tk, 7168] = C[Tk, 2048] @ W2^T [2048, 7168]\n    // Interpret w2_f32 row-major [7168, 2048] as column-major [2048, 7168], then transpose in GEMM.\n    CUBLAS_CHECK(cublasSgemm(\n        handle,\n        CUBLAS_OP_T, CUBLAS_OP_N,\n        HIDDEN_SIZE,            // m = 7168\n        Tk,                     // n = Tk\n        INTERMEDIATE_SIZE,      // k = 2048\n        &alpha,\n        w2_f32.data_ptr<float>(), INTERMEDIATE_SIZE, // A: (2048 x 7168) col-major, lda=2048, op(T)->(7168 x 2048)\n        C.data_ptr<float>(),      INTERMEDIATE_SIZE, // B: (2048 x Tk),  ldb=2048\n        &beta0,\n        Otmp.data_ptr<float>(), HIDDEN_SIZE));       // C: (7168 x Tk),  ldc=7168\n\n    // Accumulate weighted add to output\n    launch_accumulate_weighted_add(\n        Otmp.data_ptr<float>(),\n        d_token_ids_le,\n        d_token_w_le,\n        Tk, HIDDEN_SIZE,\n        output_f32.data_ptr<float>(),\n        stream);\n  }\n\n  // Destroy cuBLAS\n  CUBLAS_CHECK(cublasDestroy(handle));\n\n  // Convert to BF16 for output\n  auto output_bf16 = output_f32.to(torch::kBFloat16);\n\n  return output_bf16;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"run\", &run,\n        \"moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048 (B200-optimized)\");\n}"
    }
  ],
  "description": "gpt-5-2025-08-07 optimized kernel for moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048 (round 2, reasoning effort: high)"
}
