{
  "name": "claude-opus-4-1_cuda_53eadf",
  "definition": "gemm_n28672_k4096",
  "description": "claude-opus-4-1-20250805 optimized kernel for gemm_n28672_k4096 (round 10)",
  "author": "claude-opus-4-1-20250805",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.cpp::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "kernel.h",
      "content": "#ifndef GEMM_KERNEL_H\n#define GEMM_KERNEL_H\n\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\n// Constants for the fixed dimensions\nconstexpr int N_SIZE = 28672;\nconstexpr int K_SIZE = 4096;\n\n// Tile sizes optimized for B200 (SM90)\nconstexpr int TILE_M = 128;\nconstexpr int TILE_N = 128;\nconstexpr int TILE_K = 32;\n\n// Warp-level tile sizes for tensor cores\nconstexpr int WMMA_M = 16;\nconstexpr int WMMA_N = 16;\nconstexpr int WMMA_K = 16;\n\n// Thread block configuration\nconstexpr int WARPS_PER_BLOCK = 8;\nconstexpr int THREADS_PER_BLOCK = WARPS_PER_BLOCK * 32;\n\n// Host function declaration\nvoid launch_gemm_kernel(\n    const half* A,\n    const half* B,\n    half* C,\n    int M,\n    cudaStream_t stream\n);\n\n// Kernel declarations\n__global__ void gemm_kernel_tensorcore(\n    const half* __restrict__ A,\n    const half* __restrict__ B,\n    half* __restrict__ C,\n    int M\n);\n\n__global__ void gemm_kernel_simple(\n    const half* __restrict__ A,\n    const half* __restrict__ B,\n    half* __restrict__ C,\n    int M\n);\n\n#endif // GEMM_KERNEL_H"
    },
    {
      "path": "kernel.cu",
      "content": "#include \"kernel.h\"\n#include <mma.h>\n#include <cuda_fp16.h>\n\nusing namespace nvcuda;\n\n// Optimized tensor core kernel for B200\n__global__ void gemm_kernel_tensorcore(\n    const half* __restrict__ A,\n    const half* __restrict__ B,\n    half* __restrict__ C,\n    int M\n) {\n    // Thread and warp identifiers\n    const int warp_id = threadIdx.x / 32;\n    const int lane_id = threadIdx.x % 32;\n    \n    // Each block handles a TILE_M x TILE_N output tile\n    const int block_row = blockIdx.y;\n    const int block_col = blockIdx.x;\n    \n    // Global starting positions for this block's tile\n    const int global_row_start = block_row * TILE_M;\n    const int global_col_start = block_col * TILE_N;\n    \n    // Early exit if block is out of bounds\n    if (global_row_start >= M || global_col_start >= N_SIZE) return;\n    \n    // Shared memory for tiles\n    __shared__ half smem_A[TILE_M][TILE_K];\n    __shared__ half smem_B[TILE_N][TILE_K];\n    \n    // Each warp computes a 16x16 output tile\n    const int warps_per_row = TILE_N / WMMA_N;\n    const int warp_row = warp_id / warps_per_row;\n    const int warp_col = warp_id % warps_per_row;\n    \n    const int warp_m_offset = warp_row * WMMA_M;\n    const int warp_n_offset = warp_col * WMMA_N;\n    \n    // WMMA fragments\n    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> b_frag;\n    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;\n    \n    // Initialize accumulator\n    wmma::fill_fragment(acc_frag, 0.0f);\n    \n    // Main K loop - iterate over K dimension\n    const int num_k_tiles = (K_SIZE + TILE_K - 1) / TILE_K;\n    \n    for (int k_tile = 0; k_tile < num_k_tiles; k_tile++) {\n        const int k_start = k_tile * TILE_K;\n        \n        // Collaborative loading of A tile\n        // Each thread loads multiple elements\n        for (int idx = threadIdx.x; idx < TILE_M * TILE_K; idx += THREADS_PER_BLOCK) {\n            const int row = idx / TILE_K;\n            const int col = idx % TILE_K;\n            const int global_row = global_row_start + row;\n            const int global_col = k_start + col;\n            \n            if (global_row < M && global_col < K_SIZE) {\n                smem_A[row][col] = A[global_row * K_SIZE + global_col];\n            } else {\n                smem_A[row][col] = __float2half(0.0f);\n            }\n        }\n        \n        // Collaborative loading of B tile (B is stored as [N, K], we need B^T)\n        for (int idx = threadIdx.x; idx < TILE_N * TILE_K; idx += THREADS_PER_BLOCK) {\n            const int row = idx / TILE_K;\n            const int col = idx % TILE_K;\n            const int global_n = global_col_start + row;\n            const int global_k = k_start + col;\n            \n            if (global_n < N_SIZE && global_k < K_SIZE) {\n                smem_B[row][col] = B[global_n * K_SIZE + global_k];\n            } else {\n                smem_B[row][col] = __float2half(0.0f);\n            }\n        }\n        \n        __syncthreads();\n        \n        // Compute using tensor cores\n        for (int k = 0; k < TILE_K; k += WMMA_K) {\n            // Only process if within tile bounds\n            if (warp_m_offset < TILE_M && warp_n_offset < TILE_N) {\n                // Load A fragment\n                wmma::load_matrix_sync(a_frag, \n                    (half*)&smem_A[warp_m_offset][k], TILE_K);\n                \n                // Load B fragment (transposed)\n                wmma::load_matrix_sync(b_frag, \n                    (half*)&smem_B[warp_n_offset][k], TILE_K);\n                \n                // Perform matrix multiply-accumulate\n                wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n            }\n        }\n        \n        __syncthreads();\n    }\n    \n    // Store results back to global memory\n    if (warp_m_offset < TILE_M && warp_n_offset < TILE_N) {\n        const int global_row = global_row_start + warp_m_offset;\n        const int global_col = global_col_start + warp_n_offset;\n        \n        if (global_row < M && global_col < N_SIZE) {\n            // Convert accumulator to half\n            wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> acc_half;\n            \n            for (int i = 0; i < acc_frag.num_elements; i++) {\n                acc_half.x[i] = __float2half(acc_frag.x[i]);\n            }\n            \n            // Store with boundary checking\n            if (global_row + WMMA_M <= M && global_col + WMMA_N <= N_SIZE) {\n                wmma::store_matrix_sync(\n                    &C[global_row * N_SIZE + global_col],\n                    acc_half,\n                    N_SIZE,\n                    wmma::mem_row_major\n                );\n            } else {\n                // Handle boundary case element by element\n                half tile_result[WMMA_M * WMMA_N];\n                wmma::store_matrix_sync(tile_result, acc_half, WMMA_N, wmma::mem_row_major);\n                \n                for (int i = 0; i < WMMA_M; i++) {\n                    for (int j = 0; j < WMMA_N; j++) {\n                        if (global_row + i < M && global_col + j < N_SIZE) {\n                            C[(global_row + i) * N_SIZE + global_col + j] = tile_result[i * WMMA_N + j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Simple kernel for small matrices or fallback\n__global__ void gemm_kernel_simple(\n    const half* __restrict__ A,\n    const half* __restrict__ B,\n    half* __restrict__ C,\n    int M\n) {\n    const int BLOCK_SIZE = 16;\n    \n    __shared__ float tile_A[BLOCK_SIZE][BLOCK_SIZE + 1];\n    __shared__ float tile_B[BLOCK_SIZE][BLOCK_SIZE + 1];\n    \n    const int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    const int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    // Loop over K dimension in tiles\n    for (int k_start = 0; k_start < K_SIZE; k_start += BLOCK_SIZE) {\n        // Load A tile\n        if (row < M && k_start + threadIdx.x < K_SIZE) {\n            tile_A[threadIdx.y][threadIdx.x] = \n                __half2float(A[row * K_SIZE + k_start + threadIdx.x]);\n        } else {\n            tile_A[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        // Load B tile (B^T access pattern)\n        if (col < N_SIZE && k_start + threadIdx.y < K_SIZE) {\n            tile_B[threadIdx.y][threadIdx.x] = \n                __half2float(B[col * K_SIZE + k_start + threadIdx.y]);\n        } else {\n            tile_B[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial dot product\n        #pragma unroll\n        for (int k = 0; k < BLOCK_SIZE; k++) {\n            sum = fmaf(tile_A[threadIdx.y][k], tile_B[k][threadIdx.x], sum);\n        }\n        \n        __syncthreads();\n    }\n    \n    // Store result\n    if (row < M && col < N_SIZE) {\n        C[row * N_SIZE + col] = __float2half(sum);\n    }\n}\n\nvoid launch_gemm_kernel(\n    const half* A,\n    const half* B,\n    half* C,\n    int M,\n    cudaStream_t stream\n) {\n    // Check device capability\n    int device;\n    cudaGetDevice(&device);\n    cudaDeviceProp props;\n    cudaGetDeviceProperties(&props, device);\n    \n    // Use tensor cores for SM 7.0+ and reasonable matrix sizes\n    bool use_tensor_cores = (props.major >= 7) && (M >= WMMA_M);\n    \n    if (use_tensor_cores) {\n        // Tensor core kernel configuration\n        dim3 blockDim(THREADS_PER_BLOCK);\n        dim3 gridDim(\n            (N_SIZE + TILE_N - 1) / TILE_N,\n            (M + TILE_M - 1) / TILE_M\n        );\n        \n        // Launch tensor core kernel\n        gemm_kernel_tensorcore<<<gridDim, blockDim, 0, stream>>>(\n            A, B, C, M\n        );\n    } else {\n        // Simple kernel for smaller matrices\n        const int BLOCK_SIZE = 16;\n        dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n        dim3 gridDim(\n            (N_SIZE + BLOCK_SIZE - 1) / BLOCK_SIZE,\n            (M + BLOCK_SIZE - 1) / BLOCK_SIZE\n        );\n        \n        gemm_kernel_simple<<<gridDim, blockDim, 0, stream>>>(\n            A, B, C, M\n        );\n    }\n    \n    // Check for kernel launch errors\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        printf(\"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n    }\n}"
    },
    {
      "path": "main.cpp",
      "content": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cublas_v2.h>\n#include \"kernel.h\"\n#include <iostream>\n#include <stdexcept>\n#include <memory>\n#include <mutex>\n\n// Helper macros for error checking\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t error = call; \\\n        if (error != cudaSuccess) { \\\n            throw std::runtime_error(std::string(\"CUDA error at \") + __FILE__ + \":\" + \\\n                std::to_string(__LINE__) + \" - \" + cudaGetErrorString(error)); \\\n        } \\\n    } while(0)\n\n#define CUBLAS_CHECK(call) \\\n    do { \\\n        cublasStatus_t status = call; \\\n        if (status != CUBLAS_STATUS_SUCCESS) { \\\n            throw std::runtime_error(std::string(\"cuBLAS error at \") + __FILE__ + \":\" + \\\n                std::to_string(__LINE__) + \" code: \" + std::to_string(status)); \\\n        } \\\n    } while(0)\n\n// Thread-safe cuBLAS handle management\nclass CublasHandleManager {\nprivate:\n    cublasHandle_t handle;\n    static std::unique_ptr<CublasHandleManager> instance;\n    static std::mutex mutex;\n    \n    CublasHandleManager() {\n        CUBLAS_CHECK(cublasCreate(&handle));\n        // Enable tensor cores\n        CUBLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));\n    }\n    \npublic:\n    ~CublasHandleManager() {\n        if (handle) {\n            cublasDestroy(handle);\n        }\n    }\n    \n    static cublasHandle_t get() {\n        std::lock_guard<std::mutex> lock(mutex);\n        if (!instance) {\n            instance.reset(new CublasHandleManager());\n        }\n        return instance->handle;\n    }\n    \n    CublasHandleManager(const CublasHandleManager&) = delete;\n    CublasHandleManager& operator=(const CublasHandleManager&) = delete;\n};\n\nstd::unique_ptr<CublasHandleManager> CublasHandleManager::instance = nullptr;\nstd::mutex CublasHandleManager::mutex;\n\ntorch::Tensor run(torch::Tensor A, torch::Tensor B) {\n    // Input validation\n    TORCH_CHECK(A.dim() == 2, \"Input A must be 2-dimensional, got \", A.dim());\n    TORCH_CHECK(B.dim() == 2, \"Input B must be 2-dimensional, got \", B.dim());\n    TORCH_CHECK(A.size(1) == K_SIZE, \"A must have \", K_SIZE, \" columns, got \", A.size(1));\n    TORCH_CHECK(B.size(0) == N_SIZE, \"B must have \", N_SIZE, \" rows, got \", B.size(0));\n    TORCH_CHECK(B.size(1) == K_SIZE, \"B must have \", K_SIZE, \" columns, got \", B.size(1));\n    TORCH_CHECK(A.scalar_type() == torch::kFloat16, \"A must be float16\");\n    TORCH_CHECK(B.scalar_type() == torch::kFloat16, \"B must be float16\");\n    TORCH_CHECK(A.is_cuda(), \"A must be on CUDA device\");\n    TORCH_CHECK(B.is_cuda(), \"B must be on CUDA device\");\n    TORCH_CHECK(A.device() == B.device(), \"A and B must be on the same device\");\n    \n    // Make tensors contiguous if needed\n    torch::Tensor A_contig = A.contiguous();\n    torch::Tensor B_contig = B.contiguous();\n    \n    const int M = A_contig.size(0);\n    \n    // Create output tensor\n    auto options = torch::TensorOptions()\n        .dtype(torch::kFloat16)\n        .device(A_contig.device())\n        .requires_grad(false);\n    torch::Tensor C = torch::empty({M, N_SIZE}, options);\n    \n    // Get current CUDA stream\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n    \n    // Choose implementation based on matrix size\n    // For large matrices, cuBLAS is optimal on B200\n    if (M >= 256) {\n        // Use cuBLAS for optimal performance on large matrices\n        cublasHandle_t handle = CublasHandleManager::get();\n        CUBLAS_CHECK(cublasSetStream(handle, stream));\n        \n        const __half alpha = __float2half(1.0f);\n        const __half beta = __float2half(0.0f);\n        \n        // Compute C = A * B^T using cuBLAS\n        // We need to compute C = A * B^T\n        // In column-major view: C^T = B * A^T\n        // Since PyTorch uses row-major, we can directly compute:\n        // C(m,n) = A(m,:) * B(n,:)^T = A(m,:) * B^T(:,n)\n        \n        // Using cublasGemmEx for better performance\n        CUBLAS_CHECK(cublasGemmEx(\n            handle,\n            CUBLAS_OP_T,      // B needs to be transposed\n            CUBLAS_OP_N,      // A is not transposed\n            N_SIZE,           // m - rows of result\n            M,                // n - cols of result\n            K_SIZE,           // k - reduction dimension\n            &alpha,\n            B_contig.data_ptr<at::Half>(),  // B\n            CUDA_R_16F,       // B datatype\n            K_SIZE,           // ldb - leading dimension of B\n            A_contig.data_ptr<at::Half>(),  // A\n            CUDA_R_16F,       // A datatype\n            K_SIZE,           // lda - leading dimension of A\n            &beta,\n            C.data_ptr<at::Half>(),          // C\n            CUDA_R_16F,       // C datatype\n            N_SIZE,           // ldc - leading dimension of C\n            CUBLAS_COMPUTE_16F,  // compute type\n            CUBLAS_GEMM_DEFAULT_TENSOR_OP    // algorithm\n        ));\n    } else {\n        // Use custom kernel for smaller matrices\n        launch_gemm_kernel(\n            reinterpret_cast<const half*>(A_contig.data_ptr<at::Half>()),\n            reinterpret_cast<const half*>(B_contig.data_ptr<at::Half>()),\n            reinterpret_cast<half*>(C.data_ptr<at::Half>()),\n            M,\n            stream\n        );\n    }\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"run\", &run, \"Optimized GEMM kernel for M x 4096 @ 28672 x 4096 -> M x 28672\",\n          py::arg(\"A\"), py::arg(\"B\"));\n}"
    }
  ]
}
