{
  "name": "gemini-2.5-pro_cuda_d4c20e",
  "definition": "gemm_n28672_k4096",
  "author": "gemini-2.5-pro",
  "spec": {
    "language": "cuda",
    "target_hardware": [
      "B200"
    ],
    "entry_point": "main.cpp::run",
    "dependencies": []
  },
  "sources": [
    {
      "path": "kernel.h",
      "content": "#ifndef GEMM_N28672_K4096_KERNEL_H\n#define GEMM_N28672_K4096_KERNEL_H\n\n#include <torch/extension.h>\n\n/**\n * @brief Launches the GEMM operation C = A * B^T on the GPU.\n *\n * This function is a wrapper that sets up and calls the appropriate high-performance\n * cuBLAS kernel for the specified GEMM operation. It is optimized for the fixed\n * N and K dimensions of the problem.\n *\n * @param A The input tensor A with shape [M, K]. Must be a contiguous CUDA tensor of type float16.\n * @param B The input tensor B with shape [N, K]. Must be a contiguous CUDA tensor of type float16.\n * @param C The output tensor C with shape [M, N]. Must be a contiguous CUDA tensor of type float16.\n */\nvoid gemm_n28672_k4096_launch(\n    const torch::Tensor& A,\n    const torch::Tensor& B,\n    torch::Tensor& C);\n\n#endif // GEMM_N28672_K4096_KERNEL_H"
    },
    {
      "path": "kernel.cu",
      "content": "#include \"kernel.h\"\n#include <cublas_v2.h>\n#include <cuda_fp16.h>\n#include <stdexcept>\n#include <string>\n\n// Error checking macros for CUDA and cuBLAS calls\n#define CUDA_CHECK(call)                                                 \\\n  do {                                                                   \\\n    cudaError_t err = call;                                              \\\n    if (err != cudaSuccess) {                                            \\\n      throw std::runtime_error(\"CUDA Error: \" + std::string(cudaGetErrorString(err)) + \\\n                               \" in \" + __FILE__ + \":\" + std::to_string(__LINE__)); \\\n    }                                                                    \\\n  } while (0)\n\n#define CUBLAS_CHECK(call)                                               \\\n  do {                                                                   \\\n    cublasStatus_t status = call;                                        \\\n    if (status != CUBLAS_STATUS_SUCCESS) {                               \\\n      throw std::runtime_error(\"cuBLAS Error: status \" + std::to_string(status) + \\\n                               \" in \" + __FILE__ + \":\" + std::to_string(__LINE__)); \\\n    }                                                                    \\\n  } while (0)\n\n\n/**\n * @brief Implementation of the GEMM host launcher.\n *\n * This implementation uses the cuBLAS library, which provides highly optimized\n * matrix multiplication routines for NVIDIA GPUs. For a standard GEMM operation like this,\n * cuBLAS is the most reliable way to achieve near-peak performance on modern architectures\n * like the B200. It is expertly tuned by NVIDIA to take full advantage of hardware\n * features like Tensor Cores.\n *\n * The operation is C[M, N] = A[M, K] * B[N, K]^T.\n * This corresponds to a cublasHgemm call with:\n * - transa = CUBLAS_OP_N (A is not transposed)\n * - transb = CUBLAS_OP_T (B is transposed)\n *\n * We use cublasGemmEx for its flexibility and to ensure Tensor Core usage by specifying\n * a 32-bit float compute type for accumulation, which improves numerical stability and\n * performance.\n */\nvoid gemm_n28672_k4096_launch(\n    const torch::Tensor& A,\n    const torch::Tensor& B,\n    torch::Tensor& C) {\n\n    // Fixed dimensions from the specification\n    constexpr int64_t N = 28672;\n    constexpr int64_t K = 4096;\n\n    // Get the variable dimension M\n    const int64_t M = A.size(0);\n\n    // Get raw data pointers\n    const at::Half* a_ptr = A.data_ptr<at::Half>();\n    const at::Half* b_ptr = B.data_ptr<at::Half>();\n    at::Half* c_ptr = C.data_ptr<at::Half>();\n\n    // cuBLAS setup\n    cublasHandle_t handle;\n    CUBLAS_CHECK(cublasCreate(&handle));\n\n    // To ensure Tensor Cores are used on architectures that support them (like B200),\n    // we can set the math mode. CUBLAS_TENSOR_OP_MATH is the default on these architectures\n    // but we set it explicitly for clarity.\n    CUBLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));\n\n    // GEMM parameters\n    // The operation is C = alpha * A * B^T + beta * C\n    const float alpha = 1.0f;\n    const float beta = 0.0f;\n\n    // Leading dimensions for the matrices (since they are row-major)\n    const int64_t lda = K;\n    const int64_t ldb = K;\n    const int64_t ldc = N;\n    \n    // Note: The cuBLAS API follows Fortran's column-major convention for argument ordering.\n    // To perform C[M,N] = A[M,K] * B[K,N] in a C++/row-major world, one can express it as\n    // C^T[N,M] = B^T[N,K] * A^T[K,M].\n    // However, it's simpler to use the cublasGemmEx and keep row-major thinking, but be careful\n    // with the parameters. For C = A * B^T, where A, B, C are row-major:\n    // A -> op(A) is A, M x K\n    // B -> op(B) is B^T, K x N\n    // This matches the `cublas<t>gemm` spec directly.\n    CUBLAS_CHECK(cublasGemmEx(\n        handle,\n        CUBLAS_OP_T,          // Transpose operation for B\n        CUBLAS_OP_N,          // No transpose for A\n        N,                    // Number of rows in op(B) and C (m in cuBLAS docs)\n        M,                    // Number of columns in op(A) and C (n in cuBLAS docs)\n        K,                    // Number of columns in op(A) and rows of op(B) (k in cuBLAS docs)\n        &alpha,               // Alpha scaling factor\n        b_ptr,                // Pointer to B matrix\n        CUDA_R_16F,           // DType of B\n        ldb,                  // Leading dimension of B\n        a_ptr,                // Pointer to A matrix\n        CUDA_R_16F,           // DType of A\n        lda,                  // Leading dimension of A\n        &beta,                // Beta scaling factor\n        c_ptr,                // Pointer to C matrix\n        CUDA_R_16F,           // DType of C\n        ldc,                  // Leading dimension of C\n        CUDA_R_32F,           // Compute type (use FP32 for accumulation precision and performance)\n        CUBLAS_GEMM_DEFAULT_TENSOR_OP)); // Algorithm selection\n\n    // Clean up cuBLAS\n    CUBLAS_CHECK(cublasDestroy(handle));\n}"
    },
    {
      "path": "main.cpp",
      "content": "#include \"kernel.h\"\n#include <torch/extension.h>\n#include <vector>\n\n// Constants defined by the GEMM specification\nconstexpr int64_t N_DIM = 28672;\nconstexpr int64_t K_DIM = 4096;\n\n/**\n * @brief Python-bindable entry point for the GEMM operation.\n *\n * This function acts as a C++ interface between Python (PyTorch) and the CUDA\n * kernel launcher. It performs extensive input validation, allocates the output\n * tensor, and calls the CUDA implementation.\n *\n * @param A A PyTorch tensor representing matrix A with shape [M, 4096] and dtype float16.\n * @param B A PyTorch tensor representing matrix B with shape [28672, 4096] and dtype float16.\n * @return A new PyTorch tensor C, the result of A @ B.T, with shape [M, 28672] and dtype float16.\n */\ntorch::Tensor run(torch::Tensor A, torch::Tensor B) {\n    // --- Input Validation ---\n    TORCH_CHECK(A.dim() == 2, \"Input tensor A must be 2-dimensional\");\n    TORCH_CHECK(B.dim() == 2, \"Input tensor B must be 2-dimensional\");\n\n    TORCH_CHECK(A.is_cuda() && B.is_cuda(), \"Input tensors must be on the same CUDA device\");\n    TORCH_CHECK(A.device() == B.device(), \"Input tensors must be on the same CUDA device\");\n\n    TORCH_CHECK(A.scalar_type() == torch::kFloat16, \"Input tensor A must have dtype float16\");\n    TORCH_CHECK(B.scalar_type() == torch::kFloat16, \"Input tensor B must have dtype float16\");\n\n    TORCH_CHECK(A.size(1) == K_DIM, \"Input tensor A must have K=\", K_DIM, \", but got \", A.size(1));\n    TORCH_CHECK(B.size(0) == N_DIM, \"Input tensor B must have N=\", N_DIM, \", but got \", B.size(0));\n    TORCH_CHECK(B.size(1) == K_DIM, \"Input tensor B must have K=\", K_DIM, \", but got \", B.size(1));\n    TORCH_CHECK(A.size(1) == B.size(1), \"Inner dimensions of A and B must match (K dimension)\");\n\n    TORCH_CHECK(A.is_contiguous(), \"Input tensor A must be contiguous\");\n    TORCH_CHECK(B.is_contiguous(), \"Input tensor B must be contiguous\");\n\n    // --- Tensor Allocation ---\n    const int64_t M = A.size(0);\n    const auto C_shape = std::vector<int64_t>{M, N_DIM};\n    \n    // Create the output tensor C on the same device and with the same dtype as the inputs.\n    torch::Tensor C = torch::empty(C_shape, A.options());\n\n    // --- Kernel Execution ---\n    // Launch the CUDA kernel through the host wrapper function.\n    gemm_n28672_k4096_launch(A, B, C);\n\n    return C;\n}\n\n// --- Pybind11 Module Definition ---\n// This macro creates the Python module and binds the C++ 'run' function\n// so it can be called from Python.\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"run\", &run, \"gemm_n28672_k4096(A, B) CUDA implementation. Computes C = A @ B.T.\");\n}"
    }
  ],
  "description": "gemini-2.5-pro optimized kernel for gemm_n28672_k4096 (round 1)"
}
