Model: gpt-5-2025-08-07
================================================================================

Generated Code:
================================================================================
// File: elementwise_add.cu
#include <cuda_runtime.h>

#include <tvm/ffi/container/tensor.h>   // TensorView: tensor arguments
#include <tvm/ffi/function.h>           // TVM_FFI_DLL_EXPORT_TYPED_FUNC
#include <tvm/ffi/error.h>              // TVM_FFI_ICHECK, TVM_FFI_THROW
#include <tvm/ffi/extra/c_env_api.h>    // TVMFFIEnvGetStream

namespace elementwise {

__global__ void ElementwiseAddKernel(const float* __restrict__ a,
                                     const float* __restrict__ b,
                                     float* __restrict__ c,
                                     int64_t n) {
  int64_t idx = static_cast<int64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
  if (idx < n) {
    c[idx] = a[idx] + b[idx];
  }
}

void ElementwiseAdd(tvm::ffi::TensorView a,
                    tvm::ffi::TensorView b,
                    tvm::ffi::TensorView c) {
  // User input validation
  if (a.ndim() != 1 || b.ndim() != 1 || c.ndim() != 1) {
    TVM_FFI_THROW(ValueError) << "All tensors must be 1D, got ndim: a=" << a.ndim()
                              << ", b=" << b.ndim() << ", c=" << c.ndim();
  }

  // DType checks: float32
  auto da = a.dtype();
  auto db = b.dtype();
  auto dc = c.dtype();
  if (!(da.code == kDLFloat && da.bits == 32 && da.lanes == 1)) {
    TVM_FFI_THROW(TypeError) << "Tensor a must be float32";
  }
  if (!(db.code == kDLFloat && db.bits == 32 && db.lanes == 1)) {
    TVM_FFI_THROW(TypeError) << "Tensor b must be float32";
  }
  if (!(dc.code == kDLFloat && dc.bits == 32 && dc.lanes == 1)) {
    TVM_FFI_THROW(TypeError) << "Tensor c must be float32";
  }

  // Shape checks
  int64_t na = a.size(0);
  int64_t nb = b.size(0);
  int64_t nc = c.size(0);
  if (!(na == nb && nb == nc)) {
    TVM_FFI_THROW(ValueError) << "Shape mismatch: a.size(0)=" << na
                              << ", b.size(0)=" << nb << ", c.size(0)=" << nc;
  }

  // Device checks: must be CUDA and all on same device
  DLDevice da_dev = a.device();
  DLDevice db_dev = b.device();
  DLDevice dc_dev = c.device();
  if (!(da_dev.device_type == kDLCUDA && db_dev.device_type == kDLCUDA &&
        dc_dev.device_type == kDLCUDA)) {
    TVM_FFI_THROW(RuntimeError) << "elementwise_add supports only CUDA tensors";
  }
  if (!(da_dev.device_id == db_dev.device_id && db_dev.device_id == dc_dev.device_id)) {
    TVM_FFI_THROW(ValueError) << "All tensors must be on the same CUDA device";
  }

  // Memory layout: require contiguous for simple linear indexing
  if (!a.IsContiguous() || !b.IsContiguous() || !c.IsContiguous()) {
    TVM_FFI_THROW(ValueError) << "All tensors must be contiguous";
  }

  // Get data pointers
  const float* a_ptr = static_cast<const float*>(a.data_ptr());
  const float* b_ptr = static_cast<const float*>(b.data_ptr());
  float* c_ptr = static_cast<float*>(c.data_ptr());

  int64_t n = na;
  if (n == 0) return;

  // Get CUDA stream from TVM environment
  cudaStream_t stream = static_cast<cudaStream_t>(
      TVMFFIEnvGetStream(dc_dev.device_type, dc_dev.device_id));

  // Launch configuration
  const int threads = 256;
  unsigned int blocks = static_cast<unsigned int>((n + threads - 1) / threads);
  if (blocks == 0) return;

  // Launch kernel
  ElementwiseAddKernel<<<blocks, threads, 0, stream>>>(a_ptr, b_ptr, c_ptr, n);

  // Check for launch errors
  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    TVM_FFI_THROW(RuntimeError) << "CUDA kernel launch failed: " << cudaGetErrorString(err);
  }
}

}  // namespace elementwise

// Export the function with name "elementwise_add"
TVM_FFI_DLL_EXPORT_TYPED_FUNC(elementwise_add, elementwise::ElementwiseAdd)

Test Results:
================================================================================
Compilation: SUCCESS
Test 1 (small tensor): PASS
Test 2 (large tensor): PASS
