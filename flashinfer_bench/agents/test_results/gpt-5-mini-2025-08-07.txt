Model: gpt-5-mini-2025-08-07
================================================================================

Generated Code:
================================================================================
// File: elementwise_add.cu
#include <cstdint>
#include <cstddef>
#include <cuda_runtime.h>

#include <tvm/ffi/container/tensor.h>   // TensorView: tensor arguments
#include <tvm/ffi/extra/c_env_api.h>    // TVMFFIEnvGetStream, TVMFFIEnvTensorAlloc
#include <tvm/ffi/function.h>           // TVM_FFI_DLL_EXPORT_TYPED_FUNC
#include <tvm/ffi/error.h>              // TVM_FFI_ICHECK, TVM_FFI_THROW

using tvm::ffi::TensorView;

namespace tvm_extra_kernels {

__global__ void ElementwiseAddKernel(const float* a, const float* b, float* c, int64_t n) {
  int64_t idx = static_cast<int64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
  if (idx < n) {
    c[idx] = a[idx] + b[idx];
  }
}

void ElementwiseAdd(TensorView a, TensorView b, TensorView c) {
  // Basic shape / type validation (user-facing errors)
  if (a.ndim() != 1) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: input tensor 'a' must be 1D, got "
                              << a.ndim() << "D";
  }
  if (b.ndim() != 1) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: input tensor 'b' must be 1D, got "
                              << b.ndim() << "D";
  }
  if (c.ndim() != 1) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: output tensor 'c' must be 1D, got "
                              << c.ndim() << "D";
  }

  // dtype validation: float32
  auto da = a.dtype();
  auto db = b.dtype();
  auto dc = c.dtype();
  if (!(da.code == kDLFloat && da.bits == 32 && da.lanes == 1)) {
    TVM_FFI_THROW(TypeError) << "elementwise_add: tensor 'a' must be float32";
  }
  if (!(db.code == kDLFloat && db.bits == 32 && db.lanes == 1)) {
    TVM_FFI_THROW(TypeError) << "elementwise_add: tensor 'b' must be float32";
  }
  if (!(dc.code == kDLFloat && dc.bits == 32 && dc.lanes == 1)) {
    TVM_FFI_THROW(TypeError) << "elementwise_add: tensor 'c' must be float32";
  }

  // shape / size validation
  int64_t n = a.size(0);
  if (b.size(0) != n) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: size mismatch between 'a' and 'b' ("
                              << n << " vs " << b.size(0) << ")";
  }
  if (c.size(0) != n) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: output 'c' size must match inputs (expected "
                              << n << ", got " << c.size(0) << ")";
  }

  // Ensure tensors are contiguous for simple indexing
  if (!a.IsContiguous()) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: input 'a' must be contiguous";
  }
  if (!b.IsContiguous()) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: input 'b' must be contiguous";
  }
  if (!c.IsContiguous()) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: output 'c' must be contiguous";
  }

  // Ensure all tensors are on the same device
  DLDevice da_dev = a.device();
  DLDevice db_dev = b.device();
  DLDevice dc_dev = c.device();
  if (!(da_dev.device_type == db_dev.device_type && da_dev.device_id == db_dev.device_id &&
        da_dev.device_type == dc_dev.device_type && da_dev.device_id == dc_dev.device_id)) {
    TVM_FFI_THROW(ValueError) << "elementwise_add: tensors must be on the same device";
  }

  // Pointers to data
  float* a_ptr = static_cast<float*>(a.data_ptr());
  float* b_ptr = static_cast<float*>(b.data_ptr());
  float* c_ptr = static_cast<float*>(c.data_ptr());

  // Dispatch by device type
  if (da_dev.device_type == kDLCUDA) {
    // GPU path: launch CUDA kernel on environment-managed stream
    TVMFFIStreamHandle raw_stream = TVMFFIEnvGetStream(da_dev.device_type, da_dev.device_id);
    cudaStream_t stream = static_cast<cudaStream_t>(raw_stream);

    const int threads = 256;
    const int64_t blocks = (n + threads - 1) / threads;
    ElementwiseAddKernel<<<static_cast<int>(blocks), threads, 0, stream>>>(a_ptr, b_ptr, c_ptr, n);

    // Check kernel launch error
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
      TVM_FFI_THROW(RuntimeError) << "elementwise_add: CUDA kernel launch failed: "
                                  << cudaGetErrorString(err);
    }
    // Note: we do not synchronize here; environment may manage synchronization.
  } else if (da_dev.device_type == kDLCPU || da_dev.device_type == kDLCUDAHost) {
    // CPU path: simple element-wise loop
    for (int64_t i = 0; i < n; ++i) {
      c_ptr[i] = a_ptr[i] + b_ptr[i];
    }
  } else {
    TVM_FFI_THROW(RuntimeError) << "elementwise_add: unsupported device type "
                                << da_dev.device_type;
  }
}

// Export the function with name "elementwise_add"
TVM_FFI_DLL_EXPORT_TYPED_FUNC(elementwise_add, ElementwiseAdd);

}  // namespace tvm_extra_kernels

Test Results:
================================================================================
Compilation: SUCCESS
Test 1 (small tensor): PASS
Test 2 (large tensor): PASS
